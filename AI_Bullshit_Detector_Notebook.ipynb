{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWV9CfUDt2qRksl0Ew+bMS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvdMei/AI_Bullshit_Detector/blob/ages_branch/AI_Bullshit_Detector_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Bullshit Detector\n",
        "\n",
        "Team:\n",
        "\n",
        "Attribution:\n",
        "\n",
        "LLM mode: LiquidAI LMF2-1.2B\n",
        "\n",
        "Sundai project 11-jan-26"
      ],
      "metadata": {
        "id": "93MCDWrzMSqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lion_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uai1N9PTuvqy",
        "outputId": "43042bea-fed2-423b-e147-490f4286ec6e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lion_pytorch in /usr/local/lib/python3.12/dist-packages (0.2.3)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.12/dist-packages (from lion_pytorch) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6->lion_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6->lion_pytorch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TUIbFdcKLfUv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from lion_pytorch import Lion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create template and tokenizer for LLM model to be able to chat with it"
      ],
      "metadata": {
        "id": "CbaGATN9ejrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic question-answer template\n",
        "template_without_answer = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "template_with_answer = template_without_answer + \"{answer}<|im_end|>\\n\"\n",
        "\n",
        "# Let's try to put something into the template to see how it looks\n",
        "print(template_with_answer.format(question=\"What is your name?\", answer=\"My name is Lili!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfUiPGf4eXCy",
        "outputId": "3d6cd44b-7a3b-469f-a77d-6f4f6c53bcf3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|startoftext|><|im_start|>user\n",
            "What is your name?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "My name is Lili!<|im_end|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for Liquid AI LFM2-1.2B\n",
        "model_id = \"LiquidAI/LFM2-1.2B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# How big is the tokenizer?\n",
        "print(f\"Vocab size: {len(tokenizer.get_vocab())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GulkJu1ieeKx",
        "outputId": "d80f1a2e-cbad-4315-8711-6b85eb14dd3c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 64400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets test out both steps:\n",
        "text = \"Here is some sample text!\"\n",
        "print(f\"Original text: {text}\")\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "print(f\"Encoded tokens: {tokens}\")\n",
        "\n",
        "# Decode the tokens\n",
        "decoded_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "print(f\"Decoded text: {decoded_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq0suq7Teip2",
        "outputId": "141c954f-2e91-4948-aaa2-136dd9d98be0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: Here is some sample text!\n",
            "Encoded tokens: tensor([[   1, 9151,  856, 1429, 6643, 3304,  510]])\n",
            "Decoded text: Here is some sample text!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template_without_answer.format(question=\"What is the capital of France? Use one word.\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rouLHxijeyB_",
        "outputId": "bc9bac1a-ecc0-4e4a-996f-be362bd986ed"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|startoftext|><|im_start|>user\n",
            "What is the capital of France? Use one word.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model -- note that this may take a few minutes\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "Qulnj7PyeWpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for Liquid AI LFM2-1.2B\n",
        "model_id = \"LiquidAI/LFM2-1.2B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# How big is the tokenizer?\n",
        "print(f\"Vocab size: {len(tokenizer.get_vocab())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UYbJip4yh-3",
        "outputId": "2852300b-45dd-4570-aef4-46cae633545a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 64400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "rQSwjoG2ysXm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template_without_answer.format(question=\"What does MIT stand for?\")\n",
        "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "output = model.generate(tokens, max_new_tokens=20)\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi4ylBriuCO9",
        "outputId": "c30836d6-8d95-4179-d411-0f39b3ad934e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|startoftext|><|startoftext|><|im_start|>user\n",
            "What does MIT stand for?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "MIT stands for Massachusetts Institute of Technology. It is a private, research-intensive university located in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the internal representations of the model"
      ],
      "metadata": {
        "id": "BAZyrnmNFaeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the model structure\n",
        "print(model.config)\n",
        "print(model)\n",
        "\n",
        "# Check if the model has a specific cache class\n",
        "from transformers import Cache\n",
        "print(f\"Cache class: {getattr(model.config, 'cache_implementation', 'default')}\")"
      ],
      "metadata": {
        "id": "oO8gk8ugFaEn",
        "outputId": "213ec8b8-dcb4-4c6a-f301-0398961ade84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lfm2Config {\n",
            "  \"architectures\": [\n",
            "    \"Lfm2ForCausalLM\"\n",
            "  ],\n",
            "  \"block_auto_adjust_ff_dim\": true,\n",
            "  \"block_dim\": 2048,\n",
            "  \"block_ff_dim\": 12288,\n",
            "  \"block_ffn_dim_multiplier\": 1.0,\n",
            "  \"block_mlp_init_scale\": 1.0,\n",
            "  \"block_multiple_of\": 256,\n",
            "  \"block_norm_eps\": 1e-05,\n",
            "  \"block_out_init_scale\": 1.0,\n",
            "  \"block_use_swiglu\": true,\n",
            "  \"block_use_xavier_init\": true,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"conv_L_cache\": 3,\n",
            "  \"conv_bias\": false,\n",
            "  \"conv_dim\": 2048,\n",
            "  \"conv_dim_out\": 2048,\n",
            "  \"conv_use_xavier_init\": true,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"eos_token_id\": 7,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"layer_types\": [\n",
            "    \"conv\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 128000,\n",
            "  \"model_type\": \"lfm2\",\n",
            "  \"norm_eps\": 1e-05,\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_pos_enc\": true,\n",
            "  \"vocab_size\": 65536\n",
            "}\n",
            "\n",
            "Lfm2ForCausalLM(\n",
            "  (model): Lfm2Model(\n",
            "    (embed_tokens): Embedding(65536, 2048, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (2): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (3-4): 2 x Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (5): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (6-7): 2 x Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (8): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (9): Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (10): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (11): Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (12): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (13): Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (14): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (15): Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (rotary_emb): Lfm2RotaryEmbedding()\n",
            "    (pos_emb): Lfm2RotaryEmbedding()\n",
            "    (embedding_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=65536, bias=False)\n",
            ")\n",
            "Cache class: default\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode prompt\n",
        "prompt = template_without_answer.format(question=\"What does MIT stand for?\")\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Forward pass with cache\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids,\n",
        "        use_cache=True,\n",
        "        return_dict=True,\n",
        "        output_attentions=True,  # Optional: attention weights\n",
        "        output_hidden_states=True  # Optional: hidden states\n",
        "    )\n",
        "\n",
        "# Access cache\n",
        "past_key_values = outputs.past_key_values\n",
        "logits = outputs.logits"
      ],
      "metadata": {
        "id": "Qhru7ph8Fpdl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect cache structure\n",
        "print(f\"Cache type: {type(past_key_values)}\")\n",
        "print(f\"Number of layers in cache: {len(past_key_values)}\")\n",
        "\n",
        "# For each layer, check the structure\n",
        "for i, layer_cache in enumerate(past_key_values):\n",
        "    if layer_cache is not None:\n",
        "        if isinstance(layer_cache, tuple):\n",
        "            print(f\"Layer {i}: K shape={layer_cache[0].shape}, V shape={layer_cache[1].shape}\")\n",
        "        else:\n",
        "            print(f\"Layer {i}: type={type(layer_cache)}\")\n",
        "    else:\n",
        "        print(f\"Layer {i}: None (likely conv layer)\")"
      ],
      "metadata": {
        "id": "TK73jURaFx1O",
        "outputId": "411e62d5-5590-416d-e8dd-7445b17df2c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache type: <class 'transformers.models.lfm2.modeling_lfm2.Lfm2HybridConvCache'>\n",
            "Number of layers in cache: 15\n",
            "Layer 0: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 1: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 2: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n",
            "Layer 3: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 4: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 5: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n",
            "Layer 6: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 7: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 8: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n",
            "Layer 9: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 10: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n",
            "Layer 11: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 12: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n",
            "Layer 13: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 14: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_cache_and_probs(model, tokenizer, prompt, max_new_tokens=20):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    generated_tokens = []\n",
        "    token_probs = []\n",
        "    all_caches = []\n",
        "\n",
        "    past_key_values = None\n",
        "    current_ids = input_ids\n",
        "\n",
        "    for step in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                current_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "        # Get logits for last token\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        # Compute probabilities\n",
        "        probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "        # Sample or greedy decode\n",
        "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # Get probability of selected token\n",
        "        selected_prob = probs[0, next_token.item()].item()\n",
        "\n",
        "        # Store results\n",
        "        generated_tokens.append(next_token.item())\n",
        "        token_probs.append(selected_prob)\n",
        "        all_caches.append(outputs.past_key_values)\n",
        "\n",
        "        # Check for EOS\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        # Update for next iteration\n",
        "        past_key_values = outputs.past_key_values\n",
        "        current_ids = next_token\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"generated_tokens\": generated_tokens,\n",
        "        \"token_probs\": token_probs,\n",
        "        \"final_cache\": all_caches[-1],\n",
        "        \"text\": tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    }"
      ],
      "metadata": {
        "id": "Cxq5dcVvGDVd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run generation\n",
        "result = generate_with_cache_and_probs(\n",
        "    model, tokenizer,\n",
        "    template_without_answer.format(question=\"What does MIT stand for?\"),\n",
        "    max_new_tokens=20\n",
        ")\n",
        "\n",
        "# Display output\n",
        "print(f\"Generated text: {result['text']}\")\n",
        "print(f\"\\nToken-by-token breakdown:\")\n",
        "for i, (tok, prob) in enumerate(zip(result['generated_tokens'], result['token_probs'])):\n",
        "    print(f\"  Step {i}: '{tokenizer.decode([tok])}' (prob: {prob:.4f})\")\n",
        "\n",
        "# Analyze final KV cache\n",
        "final_cache = result['final_cache']\n",
        "print(f\"\\nFinal KV Cache Summary:\")\n",
        "for i, layer_cache in enumerate(final_cache):\n",
        "    if layer_cache is not None and isinstance(layer_cache, tuple):\n",
        "        k, v = layer_cache\n",
        "        print(f\"  Layer {i}: K={k.shape}, V={v.shape}, K_mem={k.element_size()*k.nelement()/1024:.1f}KB\")"
      ],
      "metadata": {
        "id": "Ls0jorOHGHZ5",
        "outputId": "fb7e4296-b6a8-46b0-ca8d-11f30dc67d0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: MIT stands for Massachusetts Institute of Technology. It is a private, research-intensive university located in\n",
            "\n",
            "Token-by-token breakdown:\n",
            "  Step 0: 'M' (prob: 0.9520)\n",
            "  Step 1: 'IT' (prob: 0.9993)\n",
            "  Step 2: ' stands' (prob: 0.9243)\n",
            "  Step 3: ' for' (prob: 0.9999)\n",
            "  Step 4: ' Massachusetts' (prob: 0.8794)\n",
            "  Step 5: ' Institute' (prob: 0.9990)\n",
            "  Step 6: ' of' (prob: 0.9999)\n",
            "  Step 7: ' Technology' (prob: 0.9993)\n",
            "  Step 8: '.' (prob: 0.8819)\n",
            "  Step 9: ' It' (prob: 0.8163)\n",
            "  Step 10: ' is' (prob: 0.5875)\n",
            "  Step 11: ' a' (prob: 0.8055)\n",
            "  Step 12: ' private' (prob: 0.3536)\n",
            "  Step 13: ',' (prob: 0.6832)\n",
            "  Step 14: ' research' (prob: 0.7299)\n",
            "  Step 15: '-int' (prob: 0.6954)\n",
            "  Step 16: 'ensive' (prob: 0.9982)\n",
            "  Step 17: ' university' (prob: 0.1610)\n",
            "  Step 18: ' located' (prob: 0.9285)\n",
            "  Step 19: ' in' (prob: 0.9984)\n",
            "\n",
            "Final KV Cache Summary:\n",
            "  Layer 0: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 1: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 2: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n",
            "  Layer 3: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 4: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 5: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n",
            "  Layer 6: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 7: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 8: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n",
            "  Layer 9: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 10: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n",
            "  Layer 11: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 12: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n",
            "  Layer 13: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 14: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_kv_cache(cache):\n",
        "    \"\"\"Analyze KV cache for hybrid LFM2 model\"\"\"\n",
        "    stats = {\n",
        "        \"total_layers\": len(cache),\n",
        "        \"attention_layers\": 0,\n",
        "        \"total_memory_bytes\": 0,\n",
        "        \"layer_details\": []\n",
        "    }\n",
        "\n",
        "    for i, layer_cache in enumerate(cache):\n",
        "        if layer_cache is not None and isinstance(layer_cache, tuple):\n",
        "            k, v = layer_cache\n",
        "            layer_mem = (k.element_size() * k.nelement() +\n",
        "                        v.element_size() * v.nelement())\n",
        "            stats[\"attention_layers\"] += 1\n",
        "            stats[\"total_memory_bytes\"] += layer_mem\n",
        "            stats[\"layer_details\"].append({\n",
        "                \"layer\": i,\n",
        "                \"k_shape\": list(k.shape),\n",
        "                \"v_shape\": list(v.shape),\n",
        "                \"memory_kb\": layer_mem / 1024\n",
        "            })\n",
        "\n",
        "    return stats\n",
        "\n",
        "cache_stats = analyze_kv_cache(result['final_cache'])\n",
        "print(json.dumps(cache_stats, indent=2))"
      ],
      "metadata": {
        "id": "GEwQ_ohIGORj",
        "outputId": "02a20ba3-04a5-491c-e7bd-cc67bbda55ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"total_layers\": 15,\n",
            "  \"attention_layers\": 15,\n",
            "  \"total_memory_bytes\": 860160,\n",
            "  \"layer_details\": [\n",
            "    {\n",
            "      \"layer\": 0,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 1,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 2,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 3,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 4,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 5,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 6,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 7,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 8,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 9,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 10,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 11,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 12,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 13,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 14,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Calculate the I Do Not Know Score"
      ],
      "metadata": {
        "id": "eJS6QLFqUWm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1"
      ],
      "metadata": {
        "id": "BzbnjcPSU2WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class HallucinationFeatures:\n",
        "    \"\"\"Features extracted for hallucination detection.\"\"\"\n",
        "    # Input\n",
        "    prompt: str\n",
        "    input_ids: torch.Tensor\n",
        "\n",
        "    # Hidden states (per generated token)\n",
        "    z8_states: List[torch.Tensor]   # Layer 8 hidden states\n",
        "    z12_states: List[torch.Tensor]  # Layer 12 hidden states\n",
        "\n",
        "    # KV cache stats (per generated token)\n",
        "    kv8_head_disagreement: List[float]\n",
        "    kv12_head_disagreement: List[float]\n",
        "\n",
        "    # Output\n",
        "    generated_tokens: List[int]\n",
        "    token_probs: List[float]\n",
        "    token_entropies: List[float]\n",
        "    top5_tokens: List[List[tuple]]\n",
        "    full_text: str\n",
        "\n",
        "    # Summary scores\n",
        "    mean_prob: float\n",
        "    mean_entropy: float\n",
        "    mean_head_disagreement_8: float\n",
        "    mean_head_disagreement_12: float\n",
        "\n",
        "\n",
        "def compute_head_disagreement(kv_cache_layer) -> float:\n",
        "    \"\"\"\n",
        "    Compute disagreement across KV heads for a single layer.\n",
        "    Higher = more disagreement = potential hallucination signal.\n",
        "\n",
        "    kv_cache_layer: tuple of (K, V) each with shape [batch, 8_heads, seq, 64]\n",
        "    \"\"\"\n",
        "    k, v = kv_cache_layer\n",
        "\n",
        "    if k.numel() == 0:  # Conv layer, no KV cache\n",
        "        return 0.0\n",
        "\n",
        "    # Variance across heads (dim=1) for the LAST token position\n",
        "    # Shape: [batch, heads, head_dim] -> variance over heads\n",
        "    k_last = k[:, :, -1, :]  # [1, 8, 64]\n",
        "    v_last = v[:, :, -1, :]\n",
        "\n",
        "    k_var = k_last.var(dim=1).mean().item()  # Variance across 8 heads\n",
        "    v_var = v_last.var(dim=1).mean().item()\n",
        "\n",
        "    return k_var + v_var\n",
        "\n",
        "\n",
        "def extract_hallucination_features(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 50\n",
        ") -> HallucinationFeatures:\n",
        "    \"\"\"\n",
        "    Generate tokens while extracting all features needed for hallucination detection.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Storage\n",
        "    z8_states = []\n",
        "    z12_states = []\n",
        "    kv8_disagreement = []\n",
        "    kv12_disagreement = []\n",
        "    generated_tokens = []\n",
        "    token_probs = []\n",
        "    token_entropies = []\n",
        "    top5_tokens = []\n",
        "\n",
        "    past_key_values = None\n",
        "    current_ids = input_ids\n",
        "\n",
        "    for step in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                current_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "                output_hidden_states=True,  # KEY: Get layer outputs\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "        # === HIDDEN STATES (z8, z12) ===\n",
        "        # hidden_states is tuple of (embedding, layer0, layer1, ..., layer15)\n",
        "        # So layer 8 output is at index 9, layer 12 is at index 13\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        # Get last token's hidden state for the current step\n",
        "        z8 = hidden_states[9][:, -1, :].clone().cpu()   # [1, 2048]\n",
        "        z12 = hidden_states[13][:, -1, :].clone().cpu() # [1, 2048]\n",
        "\n",
        "        z8_states.append(z8)\n",
        "        z12_states.append(z12)\n",
        "\n",
        "        # === KV CACHE HEAD DISAGREEMENT ===\n",
        "        cache = outputs.past_key_values\n",
        "        kv8_disagreement.append(compute_head_disagreement(cache[8]))\n",
        "        kv12_disagreement.append(compute_head_disagreement(cache[12]))\n",
        "\n",
        "        # === LOGITS & PROBABILITIES ===\n",
        "        logits = outputs.logits[:, -1, :]  # [1, vocab_size]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Entropy of distribution\n",
        "        entropy = -(probs * torch.log(probs + 1e-10)).sum().item()\n",
        "\n",
        "        # Greedy selection\n",
        "        next_token = torch.argmax(logits, dim=-1)\n",
        "        selected_prob = probs[0, next_token.item()].item()\n",
        "\n",
        "        # Top-5\n",
        "        top5_probs, top5_idx = torch.topk(probs[0], 5)\n",
        "        top5 = [(tokenizer.decode([idx.item()]), prob.item())\n",
        "                for idx, prob in zip(top5_idx, top5_probs)]\n",
        "\n",
        "        # Store\n",
        "        generated_tokens.append(next_token.item())\n",
        "        token_probs.append(selected_prob)\n",
        "        token_entropies.append(entropy)\n",
        "        top5_tokens.append(top5)\n",
        "\n",
        "        # Check EOS\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        # Update for next step\n",
        "        past_key_values = outputs.past_key_values\n",
        "        current_ids = next_token.unsqueeze(0)\n",
        "\n",
        "    # === BUILD RESULT ===\n",
        "    return HallucinationFeatures(\n",
        "        prompt=prompt,\n",
        "        input_ids=input_ids.cpu(),\n",
        "        z8_states=z8_states,\n",
        "        z12_states=z12_states,\n",
        "        kv8_head_disagreement=kv8_disagreement,\n",
        "        kv12_head_disagreement=kv12_disagreement,\n",
        "        generated_tokens=generated_tokens,\n",
        "        token_probs=token_probs,\n",
        "        token_entropies=token_entropies,\n",
        "        top5_tokens=top5_tokens,\n",
        "        full_text=tokenizer.decode(generated_tokens, skip_special_tokens=True),\n",
        "        mean_prob=np.mean(token_probs),\n",
        "        mean_entropy=np.mean(token_entropies),\n",
        "        mean_head_disagreement_8=np.mean(kv8_disagreement),\n",
        "        mean_head_disagreement_12=np.mean(kv12_disagreement),\n",
        "    )"
      ],
      "metadata": {
        "id": "KVQXnQWbUdNK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Unsupervised z8  z12 Predictor"
      ],
      "metadata": {
        "id": "ItPD3gesU4ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Z8toZ12Predictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Predict z12 from z8. High prediction error = out-of-support.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=2048, bottleneck_dim=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(bottleneck_dim),\n",
        "            nn.Linear(bottleneck_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(bottleneck_dim),\n",
        "            nn.Linear(bottleneck_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, z8):\n",
        "        return self.net(z8)\n",
        "\n",
        "    def prediction_error(self, z8, z12):\n",
        "        \"\"\"MSE between predicted and actual z12.\"\"\"\n",
        "        z12_pred = self.forward(z8)\n",
        "        return F.mse_loss(z12_pred, z12, reduction='none').mean(dim=-1)\n",
        "\n",
        "\n",
        "class Z8Z12LogitsPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extended predictor: z8  z12  logits_summary\n",
        "    Captures full information flow consistency.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=2048, vocab_size=65536, bottleneck_dim=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # z8  z12 predictor\n",
        "        self.z8_to_z12 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(bottleneck_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "        # z12  logits summary (top-k probs, entropy)\n",
        "        # We predict a summary, not full vocab logits\n",
        "        self.z12_to_logits_summary = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(bottleneck_dim, 6),  # [entropy, top1_prob, top2_prob, ...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z8):\n",
        "        z12_pred = self.z8_to_z12(z8)\n",
        "        logits_summary_pred = self.z12_to_logits_summary(z12_pred)\n",
        "        return z12_pred, logits_summary_pred\n",
        "\n",
        "    def compute_oos_score(self, z8, z12, logits_summary):\n",
        "        \"\"\"\n",
        "        Out-of-support score combining both prediction errors.\n",
        "        \"\"\"\n",
        "        z12_pred, logits_pred = self.forward(z8)\n",
        "\n",
        "        err_z12 = F.mse_loss(z12_pred, z12, reduction='none').mean(dim=-1)\n",
        "        err_logits = F.mse_loss(logits_pred, logits_summary, reduction='none').mean(dim=-1)\n",
        "\n",
        "        # Weighted combination\n",
        "        return 0.7 * err_z12 + 0.3 * err_logits\n",
        "\n",
        "\n",
        "class HallucinationDataset(Dataset):\n",
        "    \"\"\"Dataset for training the predictor.\"\"\"\n",
        "    def __init__(self, features_list: List[HallucinationFeatures]):\n",
        "        self.samples = []\n",
        "\n",
        "        for feat in features_list:\n",
        "            for i, (z8, z12, prob, entropy) in enumerate(zip(\n",
        "                feat.z8_states,\n",
        "                feat.z12_states,\n",
        "                feat.token_probs,\n",
        "                feat.token_entropies\n",
        "            )):\n",
        "                # Create logits summary: [entropy, top5_probs]\n",
        "                top5_probs = [p for _, p in feat.top5_tokens[i]]\n",
        "                logits_summary = torch.tensor([entropy] + top5_probs, dtype=torch.float32)\n",
        "\n",
        "                self.samples.append({\n",
        "                    'z8': z8.squeeze(0),\n",
        "                    'z12': z12.squeeze(0),\n",
        "                    'logits_summary': logits_summary,\n",
        "                    'prob': prob,\n",
        "                    'entropy': entropy,\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "def train_predictor(features_list: List[HallucinationFeatures], epochs=10, lr=1e-4):\n",
        "    \"\"\"Train the z8  z12  logits predictor.\"\"\"\n",
        "    dataset = HallucinationDataset(features_list)\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    predictor = Z8Z12LogitsPredictor()\n",
        "    optimizer = torch.optim.AdamW(predictor.parameters(), lr=lr)\n",
        "\n",
        "    predictor.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in loader:\n",
        "            z8 = batch['z8']\n",
        "            z12 = batch['z12']\n",
        "            logits_summary = batch['logits_summary']\n",
        "\n",
        "            z12_pred, logits_pred = predictor(z8)\n",
        "\n",
        "            loss_z12 = F.mse_loss(z12_pred, z12)\n",
        "            loss_logits = F.mse_loss(logits_pred, logits_summary)\n",
        "            loss = 0.7 * loss_z12 + 0.3 * loss_logits\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(loader):.6f}\")\n",
        "\n",
        "    return predictor"
      ],
      "metadata": {
        "id": "R8oh2sWOUzQJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Head Disagreement Score (Refined)"
      ],
      "metadata": {
        "id": "xiskv8KcVAzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sequence_head_disagreement(features: HallucinationFeatures) -> Dict:\n",
        "    \"\"\"\n",
        "    Compute detailed head disagreement statistics.\n",
        "    \"\"\"\n",
        "    kv8 = np.array(features.kv8_head_disagreement)\n",
        "    kv12 = np.array(features.kv12_head_disagreement)\n",
        "\n",
        "    return {\n",
        "        # Layer 8\n",
        "        \"kv8_mean\": kv8.mean(),\n",
        "        \"kv8_std\": kv8.std(),\n",
        "        \"kv8_max\": kv8.max(),\n",
        "        \"kv8_trend\": np.polyfit(range(len(kv8)), kv8, 1)[0] if len(kv8) > 1 else 0,\n",
        "\n",
        "        # Layer 12\n",
        "        \"kv12_mean\": kv12.mean(),\n",
        "        \"kv12_std\": kv12.std(),\n",
        "        \"kv12_max\": kv12.max(),\n",
        "        \"kv12_trend\": np.polyfit(range(len(kv12)), kv12, 1)[0] if len(kv12) > 1 else 0,\n",
        "\n",
        "        # Cross-layer\n",
        "        \"kv_ratio\": kv12.mean() / (kv8.mean() + 1e-8),\n",
        "        \"kv_correlation\": np.corrcoef(kv8, kv12)[0, 1] if len(kv8) > 1 else 0,\n",
        "    }"
      ],
      "metadata": {
        "id": "qq5_nzH1VC5y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Integration with Paraphrase Dataset"
      ],
      "metadata": {
        "id": "EIavMpA8VFUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_full_hallucination_analysis(model, tokenizer, csv_path: str):\n",
        "    \"\"\"\n",
        "    Run complete analysis pipeline on paraphrase dataset.\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # === STEP 1: Extract features for all questions ===\n",
        "    print(\"Extracting internal features...\")\n",
        "    all_features = []\n",
        "    for question in tqdm(df['question'], desc=\"Processing\"):\n",
        "        prompt = template_without_answer.format(question=question)\n",
        "        features = extract_hallucination_features(model, tokenizer, prompt, max_new_tokens=100)\n",
        "        all_features.append(features)\n",
        "\n",
        "    df['llm_output'] = [f.full_text for f in all_features]\n",
        "    df['mean_prob'] = [f.mean_prob for f in all_features]\n",
        "    df['mean_entropy'] = [f.mean_entropy for f in all_features]\n",
        "    df['head_disagree_8'] = [f.mean_head_disagreement_8 for f in all_features]\n",
        "    df['head_disagree_12'] = [f.mean_head_disagreement_12 for f in all_features]\n",
        "\n",
        "    # === STEP 2: Train z8  z12 predictor ===\n",
        "    print(\"\\nTraining z8  z12 predictor...\")\n",
        "    predictor = train_predictor(all_features, epochs=20)\n",
        "\n",
        "    # === STEP 3: Compute OOS scores ===\n",
        "    print(\"\\nComputing out-of-support scores...\")\n",
        "    predictor.eval()\n",
        "    oos_scores = []\n",
        "\n",
        "    for feat in all_features:\n",
        "        scores = []\n",
        "        for z8, z12, prob, entropy in zip(\n",
        "            feat.z8_states, feat.z12_states,\n",
        "            feat.token_probs, feat.token_entropies\n",
        "        ):\n",
        "            top5_probs = [p for _, p in feat.top5_tokens[feat.z8_states.index(z8)]]\n",
        "            logits_summary = torch.tensor([entropy] + top5_probs[:5])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                score = predictor.compute_oos_score(\n",
        "                    z8.squeeze(0).unsqueeze(0),\n",
        "                    z12.squeeze(0).unsqueeze(0),\n",
        "                    logits_summary.unsqueeze(0)\n",
        "                )\n",
        "            scores.append(score.item())\n",
        "\n",
        "        oos_scores.append(np.mean(scores))\n",
        "\n",
        "    df['oos_score'] = oos_scores\n",
        "\n",
        "    # === STEP 4: Paraphrase consistency (TF-IDF) ===\n",
        "    print(\"\\nComputing paraphrase consistency...\")\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    embeddings = vectorizer.fit_transform(df['llm_output']).toarray()\n",
        "    embeddings = normalize(embeddings)\n",
        "    cos_sim_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    # Group by category and compute consistency\n",
        "    paraphrase_stats = []\n",
        "    for idx, row in df.iterrows():\n",
        "        category = row['category']\n",
        "        same_category = df[df['category'] == category].index.tolist()\n",
        "\n",
        "        if len(same_category) > 1:\n",
        "            sims = [cos_sim_matrix[idx, j] for j in same_category if j != idx]\n",
        "            paraphrase_stats.append({\n",
        "                'mean_sim': np.mean(sims),\n",
        "                'std_sim': np.std(sims),\n",
        "                'min_sim': np.min(sims),\n",
        "            })\n",
        "        else:\n",
        "            paraphrase_stats.append({'mean_sim': 1.0, 'std_sim': 0.0, 'min_sim': 1.0})\n",
        "\n",
        "    df['paraphrase_mean_sim'] = [s['mean_sim'] for s in paraphrase_stats]\n",
        "    df['paraphrase_std_sim'] = [s['std_sim'] for s in paraphrase_stats]\n",
        "    df['paraphrase_min_sim'] = [s['min_sim'] for s in paraphrase_stats]\n",
        "\n",
        "    return df, predictor, all_features\n",
        "\n",
        "\n",
        "# === RUN ANALYSIS ===\n",
        "df_results, predictor, all_features = run_full_hallucination_analysis(\n",
        "    model, tokenizer,\n",
        "    \"AI_Bullshit_Detector/data/paraphase-prompts.csv\"\n",
        ")"
      ],
      "metadata": {
        "id": "YZl-ZBdbVGv1",
        "outputId": "46062df3-a9fc-4a93-b346-abaf5dd8c6f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting internal features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing:   4%|         | 4/100 [00:23<09:17,  5.81s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine similary of paraphrased sentences --> used in testing the Iinternal representation model (IRM) and output."
      ],
      "metadata": {
        "id": "q45u8Z1uOW31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# --- 1. Load CSV ---\n",
        "#!rm -rf AI_Bullshit_Detector\n",
        "# Clone the repo\n",
        "!git clone https://github.com/AvdMei/AI_Bullshit_Detector\n",
        "df = pd.read_csv(\"AI_Bullshit_Detector/data/paraphase-prompts.csv\")\n",
        "print(df.head())\n",
        "assert \"category\" in df.columns and \"question\" in df.columns\n",
        "# --- 2. Load Liquid AI model and tokenizer ---\n",
        "model_id = \"LiquidAI/LFM2-1.2B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "model.eval()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "# --- 3. Templates ---\n",
        "template_without_answer = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "# --- 4. Function to generate response ---\n",
        "def generate_llm_output(question, max_tokens=200):\n",
        "    prompt = template_without_answer.format(question=question)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False  # deterministic for cosine similarity\n",
        "        )\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    # Remove the prompt part to get only the assistant answer\n",
        "    if \"<|im_start|>assistant\" in output_text:\n",
        "        answer = output_text.split(\"<|im_start|>assistant\")[1].strip()\n",
        "    else:\n",
        "        answer = output_text\n",
        "    return answer\n",
        "# --- 5. Generate LLM outputs ---\n",
        "llm_outputs = []\n",
        "for q in tqdm(df['question'], desc=\"Generating outputs\"):\n",
        "    try:\n",
        "        out = generate_llm_output(q)\n",
        "    except Exception as e:\n",
        "        out = f\"ERROR: {e}\"\n",
        "    llm_outputs.append(out)\n",
        "df['llm_output'] = llm_outputs\n",
        "# --- 6. Convert outputs to embeddings using TF-IDF ---\n",
        "vectorizer = TfidfVectorizer()\n",
        "embeddings = vectorizer.fit_transform(df['llm_output']).toarray()\n",
        "embeddings = normalize(embeddings)\n",
        "# --- 7. Compute cosine similarity ---\n",
        "cos_sim_matrix = cosine_similarity(embeddings)\n",
        "# --- 8. Save similarity matrix to CSV ---\n",
        "sim_df = pd.DataFrame(cos_sim_matrix, index=df['question'], columns=df['question'])\n",
        "sim_df.to_csv(\"AI_Bullshit_Detector/data/llm_output_cosine_similarity.csv\")\n",
        "print(\"Cosine similarity matrix saved to data/llm_output_cosine_similarity.csv\")\n",
        "# --- 9. Save outputs alongside prompts ---\n",
        "df.to_csv(\"AI_Bullshit_Detector/data/paraphases_with_llm_output.csv\", index=False)\n",
        "print(\"LLM outputs saved to data/paraphases_with_llm_output.csv\")"
      ],
      "metadata": {
        "id": "kHJk5UVnNaB3",
        "outputId": "8e2e2a29-dc83-4e73-89bd-bedef8c6e724",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI_Bullshit_Detector'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 139 (delta 46), reused 92 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (139/139), 457.96 KiB | 7.27 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "                           category  \\\n",
            "0  Software Engineering - Debugging   \n",
            "1  Software Engineering - Debugging   \n",
            "2  Software Engineering - Debugging   \n",
            "3  Software Engineering - Debugging   \n",
            "4  Software Engineering - Debugging   \n",
            "\n",
            "                                            question  \n",
            "0  What is the first step an experienced engineer...  \n",
            "1  When a bug is reported, what should be done fi...  \n",
            "2  What is the initial action a senior engineer t...  \n",
            "3  Before changing code to fix a bug, what should...  \n",
            "4  What is the most important first step in a pro...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Generating outputs: 100%|| 100/100 [07:49<00:00,  4.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity matrix saved to data/llm_output_cosine_similarity.csv\n",
            "LLM outputs saved to data/paraphases_with_llm_output.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Liquid AI's LFM2-1.2B"
      ],
      "metadata": {
        "id": "VN75O3XvMHeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rxTA-C69LjxM"
      }
    }
  ]
}