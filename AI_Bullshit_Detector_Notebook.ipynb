{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP/cn5koyPNBobTuYBs5clJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "973e89caff714145bee3de3237e7abb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70e1d607c07f416f997a435461b254df",
              "IPY_MODEL_6e07a7164d3848d7a7ca52df8f3ec09f",
              "IPY_MODEL_3709fb5914bd4344a433d8c33ccb927f"
            ],
            "layout": "IPY_MODEL_d5dad51295f9474397001862a4ed862c"
          }
        },
        "70e1d607c07f416f997a435461b254df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_904c690d24c3480195213a422ac9573d",
            "placeholder": "​",
            "style": "IPY_MODEL_8c9fec93e4cb462fa32372ec26484740",
            "value": "tokenizer_config.json: "
          }
        },
        "6e07a7164d3848d7a7ca52df8f3ec09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a4bbf38f05c46f2b981ad898fda4363",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfc424fba3d2459c8f41de6cffcf4b13",
            "value": 1
          }
        },
        "3709fb5914bd4344a433d8c33ccb927f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d24153e1235248bf959410ca1a531c6c",
            "placeholder": "​",
            "style": "IPY_MODEL_c659f57965a345d1acdd57d1cca822e1",
            "value": " 91.6k/? [00:00&lt;00:00, 6.23MB/s]"
          }
        },
        "d5dad51295f9474397001862a4ed862c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "904c690d24c3480195213a422ac9573d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c9fec93e4cb462fa32372ec26484740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a4bbf38f05c46f2b981ad898fda4363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cfc424fba3d2459c8f41de6cffcf4b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d24153e1235248bf959410ca1a531c6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c659f57965a345d1acdd57d1cca822e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52fb4bbba94c499588ffb8fa5ba9460d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94e6f9d0f8c64732bcb413b5c0843d1c",
              "IPY_MODEL_22c8288115224fdda8ebaa0eac50f068",
              "IPY_MODEL_d374d126503a4a3abb061a9e5855f7ec"
            ],
            "layout": "IPY_MODEL_98878be7b7ea48c1807eead441a1c250"
          }
        },
        "94e6f9d0f8c64732bcb413b5c0843d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f7ef74734c74f189078fe010d20d29e",
            "placeholder": "​",
            "style": "IPY_MODEL_c7e03561c7244b47afbd7bf80482b764",
            "value": "tokenizer.json: "
          }
        },
        "22c8288115224fdda8ebaa0eac50f068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecdef0ddf3a74550a208570c15eff343",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0969f12310854a709b4b555fe50b9499",
            "value": 1
          }
        },
        "d374d126503a4a3abb061a9e5855f7ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b2057361e0045a287f3980cd2e71eb0",
            "placeholder": "​",
            "style": "IPY_MODEL_98bdec6e10844a74b8682685a388af3a",
            "value": " 4.73M/? [00:00&lt;00:00, 161MB/s]"
          }
        },
        "98878be7b7ea48c1807eead441a1c250": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f7ef74734c74f189078fe010d20d29e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7e03561c7244b47afbd7bf80482b764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecdef0ddf3a74550a208570c15eff343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0969f12310854a709b4b555fe50b9499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b2057361e0045a287f3980cd2e71eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98bdec6e10844a74b8682685a388af3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1c047886af44406b5aa320a2f5b70d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7048187a9d4a47728de7a48d4f41a53c",
              "IPY_MODEL_e876fc68d7c04a07892057ae2e8815aa",
              "IPY_MODEL_20b2db4981f94bfc86f8c12f785edabe"
            ],
            "layout": "IPY_MODEL_ace476d3ae944ccfb0419aae9595f048"
          }
        },
        "7048187a9d4a47728de7a48d4f41a53c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e152ff46cb0e4d7a8b07c7c4ef2f5eb0",
            "placeholder": "​",
            "style": "IPY_MODEL_7f897b402ba44013847597b865b91f0e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "e876fc68d7c04a07892057ae2e8815aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efef9d7afebd4a22bd94fd19408565a6",
            "max": 434,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c70110a6f874a7796b5d692cf06ccfe",
            "value": 434
          }
        },
        "20b2db4981f94bfc86f8c12f785edabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7295505377ae4ace95484d872cb9bd0a",
            "placeholder": "​",
            "style": "IPY_MODEL_fe91d15f2ede470098158b52897a4cb7",
            "value": " 434/434 [00:00&lt;00:00, 57.2kB/s]"
          }
        },
        "ace476d3ae944ccfb0419aae9595f048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e152ff46cb0e4d7a8b07c7c4ef2f5eb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f897b402ba44013847597b865b91f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efef9d7afebd4a22bd94fd19408565a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c70110a6f874a7796b5d692cf06ccfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7295505377ae4ace95484d872cb9bd0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe91d15f2ede470098158b52897a4cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b83598bbca54caf9d88d6b36a3b92f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afdf5f5a66274f16a8251e9b47bbf28b",
              "IPY_MODEL_59e5116d4e894695bf15054df4667ee2",
              "IPY_MODEL_af8f0aa77b904d4ead41bcfbd5c22466"
            ],
            "layout": "IPY_MODEL_9848674edb9d459aae3e988b043e566e"
          }
        },
        "afdf5f5a66274f16a8251e9b47bbf28b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a57894a481584922aa5457e3ed624979",
            "placeholder": "​",
            "style": "IPY_MODEL_68e432edf76c4fd7b091026cd748005e",
            "value": "chat_template.jinja: "
          }
        },
        "59e5116d4e894695bf15054df4667ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e36cf92d0e894ba1b2fe61f8a13caf02",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_673e35666fb44d9088c1246997fa8bd1",
            "value": 1
          }
        },
        "af8f0aa77b904d4ead41bcfbd5c22466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3464fe74ab8b4aa7b0c1e761c1632bef",
            "placeholder": "​",
            "style": "IPY_MODEL_c5158f8f97604fbdb45ffe3b920a3ba6",
            "value": " 1.38k/? [00:00&lt;00:00, 171kB/s]"
          }
        },
        "9848674edb9d459aae3e988b043e566e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a57894a481584922aa5457e3ed624979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68e432edf76c4fd7b091026cd748005e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e36cf92d0e894ba1b2fe61f8a13caf02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "673e35666fb44d9088c1246997fa8bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3464fe74ab8b4aa7b0c1e761c1632bef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5158f8f97604fbdb45ffe3b920a3ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c235ba61d2b54a3a98879253f572f74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5fae5feb31eb4e0b92264a92ed15da63",
              "IPY_MODEL_eec6f22b267648909855c669817f7cd2",
              "IPY_MODEL_a9fb171a06a34867be942803db964973"
            ],
            "layout": "IPY_MODEL_67db54a07d6346a4858bc1fe107ddf63"
          }
        },
        "5fae5feb31eb4e0b92264a92ed15da63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7051f65e2f943c2830d2e5ab497067f",
            "placeholder": "​",
            "style": "IPY_MODEL_7497d28b47a44f44a2b38b62fcc7fa56",
            "value": "config.json: "
          }
        },
        "eec6f22b267648909855c669817f7cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fddaab06a30649ac84bc737496faed82",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0636fb3a593e4275b68703f236d62525",
            "value": 1
          }
        },
        "a9fb171a06a34867be942803db964973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f7b4b4e47a04d79b9a5ae3df1f47ab5",
            "placeholder": "​",
            "style": "IPY_MODEL_f3c23b13c02b4f939936619df0ffebbf",
            "value": " 1.00k/? [00:00&lt;00:00, 127kB/s]"
          }
        },
        "67db54a07d6346a4858bc1fe107ddf63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7051f65e2f943c2830d2e5ab497067f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7497d28b47a44f44a2b38b62fcc7fa56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fddaab06a30649ac84bc737496faed82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0636fb3a593e4275b68703f236d62525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f7b4b4e47a04d79b9a5ae3df1f47ab5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3c23b13c02b4f939936619df0ffebbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3720672ffec468da5fff92d4fdba22f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b1546a15e6c4e0ea44b91506f397703",
              "IPY_MODEL_6dfea70f95044878a81ecc159547de0c",
              "IPY_MODEL_8e1aa11f3a7c4a1e914e488f0542ab21"
            ],
            "layout": "IPY_MODEL_54a8e9c7ced543f6b08100b9881ebf8e"
          }
        },
        "5b1546a15e6c4e0ea44b91506f397703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbe7afbebe684807a0f5ca43c390886e",
            "placeholder": "​",
            "style": "IPY_MODEL_8a3fbc04cf454417b888119513b3722e",
            "value": "model.safetensors: 100%"
          }
        },
        "6dfea70f95044878a81ecc159547de0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97cc8de86809447ea002bcbcd9092265",
            "max": 2340697936,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6aa061768fcf4bd89810fd218edc1a04",
            "value": 2340697936
          }
        },
        "8e1aa11f3a7c4a1e914e488f0542ab21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3d287d2dfed4bdfac977d1c50b021d7",
            "placeholder": "​",
            "style": "IPY_MODEL_e3b95eaf4a304da9b4bec80aa6c4be46",
            "value": " 2.34G/2.34G [00:04&lt;00:00, 1.02GB/s]"
          }
        },
        "54a8e9c7ced543f6b08100b9881ebf8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbe7afbebe684807a0f5ca43c390886e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a3fbc04cf454417b888119513b3722e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97cc8de86809447ea002bcbcd9092265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aa061768fcf4bd89810fd218edc1a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3d287d2dfed4bdfac977d1c50b021d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3b95eaf4a304da9b4bec80aa6c4be46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "970b521546104d379d552f8c999ab8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1975897c693344869e506d24542bda64",
              "IPY_MODEL_27fdd93f02c449ab8c27de1dbb93d08c",
              "IPY_MODEL_cfbaa5b8fbf348368701a3bf5f9b20b8"
            ],
            "layout": "IPY_MODEL_1c7ed2748e544c9f98a438b881c1395e"
          }
        },
        "1975897c693344869e506d24542bda64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48959ce0f12e45eb8733121294e05480",
            "placeholder": "​",
            "style": "IPY_MODEL_f099fda6d70040e4ba7a2abc03cc0ded",
            "value": "generation_config.json: 100%"
          }
        },
        "27fdd93f02c449ab8c27de1dbb93d08c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95900df09dc34e10b5f9aef88833c789",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c24b062860c44dc85254bc0493c53a0",
            "value": 137
          }
        },
        "cfbaa5b8fbf348368701a3bf5f9b20b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bc1a5fb5b8b4f4c8da216d51e310b2a",
            "placeholder": "​",
            "style": "IPY_MODEL_45a0116e3e234eb98d7d6d55533550ef",
            "value": " 137/137 [00:00&lt;00:00, 16.2kB/s]"
          }
        },
        "1c7ed2748e544c9f98a438b881c1395e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48959ce0f12e45eb8733121294e05480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f099fda6d70040e4ba7a2abc03cc0ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95900df09dc34e10b5f9aef88833c789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c24b062860c44dc85254bc0493c53a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bc1a5fb5b8b4f4c8da216d51e310b2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a0116e3e234eb98d7d6d55533550ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvdMei/AI_Bullshit_Detector/blob/ages_branch/AI_Bullshit_Detector_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Bullshit Detector\n",
        "\n",
        "Team: Age, Anika, Nate\n",
        "\n",
        "Attribution: LiquidAI (model builder), HuggingFace (hosting)\n",
        "\n",
        "LLM mode: LiquidAI LMF2-1.2B\n",
        "\n",
        "Sundai project 11-jan-26"
      ],
      "metadata": {
        "id": "93MCDWrzMSqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# LFM2 + IDK HEAD - FIXED COLAB NOTEBOOK\n",
        "# =============================================================================\n",
        "# Copy each cell into Google Colab to run the complete pipeline\n",
        "# FIXED: Recursion error by separating base model loading from wrapper\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 1: Install dependencies (run this first)\n",
        "# =============================================================================\n",
        "# !pip install -q transformers>=4.55.0 torch tqdm\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 2: Imports and Setup\n",
        "# =============================================================================\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, Dict, List\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers.utils import ModelOutput\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 3: Output Dataclass\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class Lfm2IDKOutput(ModelOutput):\n",
        "    \"\"\"Output with IDK score.\"\"\"\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: Optional[torch.FloatTensor] = None\n",
        "    idk_score: Optional[torch.FloatTensor] = None\n",
        "    idk_components: Optional[dict] = None\n",
        "    past_key_values: Optional[Tuple] = None\n",
        "    hidden_states: Optional[Tuple] = None\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 4: IDK Head Components\n",
        "# =============================================================================\n",
        "\n",
        "class FlowPredictor(nn.Module):\n",
        "    \"\"\"Predicts z12 from z8. High error = out-of-distribution.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int = 2048, bottleneck_dim: int = 512, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(bottleneck_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(bottleneck_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(bottleneck_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(bottleneck_dim, hidden_dim),\n",
        "        )\n",
        "        self.error_scale = nn.Parameter(torch.tensor(1.0))\n",
        "        self.error_bias = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, z8: torch.Tensor) -> torch.Tensor:\n",
        "        return self.predictor(z8)\n",
        "\n",
        "    def compute_error(self, z8: torch.Tensor, z12: torch.Tensor) -> torch.Tensor:\n",
        "        z12_pred = self.forward(z8)\n",
        "        mse = F.mse_loss(z12_pred, z12, reduction='none').mean(dim=-1)\n",
        "        return torch.sigmoid(self.error_scale * mse + self.error_bias)\n",
        "\n",
        "\n",
        "class HeadDisagreementModule(nn.Module):\n",
        "    \"\"\"Computes KV head disagreement. High variance = uncertainty.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "        self.bias = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, kv_8: Tuple, kv_12: Tuple) -> torch.Tensor:\n",
        "        d8 = self._compute_disagreement(kv_8)\n",
        "        d12 = self._compute_disagreement(kv_12)\n",
        "        avg = (d8 + d12) / 2\n",
        "        return torch.sigmoid(self.scale * avg + self.bias)\n",
        "\n",
        "    def _compute_disagreement(self, kv: Tuple) -> torch.Tensor:\n",
        "        k, v = kv\n",
        "        if k.numel() == 0:\n",
        "            return torch.zeros(1, device=k.device)\n",
        "        k_last = k[:, :, -1, :].float()\n",
        "        v_last = v[:, :, -1, :].float()\n",
        "        return k_last.var(dim=1).mean(dim=-1) + v_last.var(dim=1).mean(dim=-1)\n",
        "\n",
        "\n",
        "class EntropyModule(nn.Module):\n",
        "    \"\"\"Computes normalized output entropy.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int = 65536):\n",
        "        super().__init__()\n",
        "        self.max_entropy = torch.log(torch.tensor(float(vocab_size)))\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "        self.bias = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
        "        if logits.dim() == 3:\n",
        "            logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits.float(), dim=-1)\n",
        "        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=-1)\n",
        "        normalized = entropy / self.max_entropy.to(logits.device)\n",
        "        return torch.sigmoid(self.scale * normalized + self.bias)\n",
        "\n",
        "\n",
        "class IDKHead(nn.Module):\n",
        "    \"\"\"\n",
        "    IDK Head: Combines flow prediction, head disagreement, and entropy\n",
        "    into a single uncertainty score from 0-100.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int = 2048,\n",
        "        bottleneck_dim: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        vocab_size: int = 65536,\n",
        "        flow_weight: float = 0.4,\n",
        "        head_weight: float = 0.3,\n",
        "        entropy_weight: float = 0.3\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.flow_predictor = FlowPredictor(hidden_dim, bottleneck_dim, dropout)\n",
        "        self.head_disagreement = HeadDisagreementModule()\n",
        "        self.entropy_module = EntropyModule(vocab_size)\n",
        "\n",
        "        self.flow_weight = flow_weight\n",
        "        self.head_weight = head_weight\n",
        "        self.entropy_weight = entropy_weight\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        z8: torch.Tensor = None,\n",
        "        z12: torch.Tensor = None,\n",
        "        kv_cache_8: Tuple = None,\n",
        "        kv_cache_12: Tuple = None,\n",
        "        logits: torch.Tensor = None\n",
        "    ) -> Tuple[torch.Tensor, dict]:\n",
        "\n",
        "        components = {}\n",
        "        signals = []\n",
        "        weights = []\n",
        "\n",
        "        # 1. Flow prediction error\n",
        "        if z8 is not None and z12 is not None:\n",
        "            flow_error = self.flow_predictor.compute_error(z8, z12)\n",
        "            components['flow_error'] = flow_error.item() if flow_error.dim() == 0 else flow_error[0].item()\n",
        "            signals.append(flow_error)\n",
        "            weights.append(self.flow_weight)\n",
        "\n",
        "        # 2. Head disagreement\n",
        "        if kv_cache_8 is not None and kv_cache_12 is not None:\n",
        "            head_disagree = self.head_disagreement(kv_cache_8, kv_cache_12)\n",
        "            components['head_disagreement'] = head_disagree.item() if head_disagree.dim() == 0 else head_disagree[0].item()\n",
        "            signals.append(head_disagree)\n",
        "            weights.append(self.head_weight)\n",
        "\n",
        "        # 3. Entropy\n",
        "        if logits is not None:\n",
        "            entropy_signal = self.entropy_module(logits)\n",
        "            components['entropy_signal'] = entropy_signal.item() if entropy_signal.dim() == 0 else entropy_signal[0].item()\n",
        "            signals.append(entropy_signal)\n",
        "            weights.append(self.entropy_weight)\n",
        "\n",
        "        # Combine signals\n",
        "        if len(signals) == 0:\n",
        "            return torch.tensor([50.0]), components\n",
        "\n",
        "        # Normalize weights\n",
        "        total_weight = sum(weights)\n",
        "        weights = [w / total_weight for w in weights]\n",
        "\n",
        "        # Weighted sum\n",
        "        combined = sum(s * w for s, w in zip(signals, weights))\n",
        "        idk_score = combined * 100\n",
        "\n",
        "        return idk_score, components\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 5: Wrapper Class (Simplified - avoids recursion)\n",
        "# =============================================================================\n",
        "\n",
        "class LFM2WithIDK(nn.Module):\n",
        "    \"\"\"\n",
        "    LFM2 with IDK uncertainty head.\n",
        "\n",
        "    This is a simple nn.Module wrapper (not PreTrainedModel) to avoid\n",
        "    recursion issues. For HuggingFace Hub hosting, use the full package version.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model,  # Pass pre-loaded model\n",
        "        hidden_dim: int = 2048,\n",
        "        bottleneck_dim: int = 512,\n",
        "        layer_8_idx: int = 8,\n",
        "        layer_12_idx: int = 12,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store base model (already loaded)\n",
        "        self.lm = base_model\n",
        "\n",
        "        # Freeze base model\n",
        "        for param in self.lm.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # IDK Head\n",
        "        self.idk_head = IDKHead(\n",
        "            hidden_dim=hidden_dim,\n",
        "            bottleneck_dim=bottleneck_dim,\n",
        "        )\n",
        "\n",
        "        # Layer indices (hidden_states[0] is embeddings, so +1)\n",
        "        self.z8_idx = layer_8_idx + 1  # 9\n",
        "        self.z12_idx = layer_12_idx + 1  # 13\n",
        "        self.layer_8_idx = layer_8_idx\n",
        "        self.layer_12_idx = layer_12_idx\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor = None,\n",
        "        past_key_values = None,\n",
        "        output_idk_score: bool = True,\n",
        "    ) -> Lfm2IDKOutput:\n",
        "\n",
        "        # Forward through base model\n",
        "        outputs = self.lm(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=True,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        idk_score = None\n",
        "        idk_components = None\n",
        "\n",
        "        if output_idk_score and outputs.hidden_states is not None:\n",
        "            # Extract z8, z12\n",
        "            z8 = outputs.hidden_states[self.z8_idx][:, -1, :].float()\n",
        "            z12 = outputs.hidden_states[self.z12_idx][:, -1, :].float()\n",
        "\n",
        "            # Extract KV caches\n",
        "            kv_cache_8, kv_cache_12 = None, None\n",
        "\n",
        "            if outputs.past_key_values is not None:\n",
        "                cache = outputs.past_key_values\n",
        "                try:\n",
        "                    kv_8 = cache[self.layer_8_idx]\n",
        "                    kv_12 = cache[self.layer_12_idx]\n",
        "\n",
        "                    if isinstance(kv_8, tuple) and len(kv_8) >= 2:\n",
        "                        if kv_8[0].numel() > 0:\n",
        "                            kv_cache_8 = (kv_8[0].float(), kv_8[1].float())\n",
        "\n",
        "                    if isinstance(kv_12, tuple) and len(kv_12) >= 2:\n",
        "                        if kv_12[0].numel() > 0:\n",
        "                            kv_cache_12 = (kv_12[0].float(), kv_12[1].float())\n",
        "                except (IndexError, TypeError, AttributeError):\n",
        "                    pass\n",
        "\n",
        "            # Compute IDK score\n",
        "            idk_score, idk_components = self.idk_head(\n",
        "                z8=z8,\n",
        "                z12=z12,\n",
        "                kv_cache_8=kv_cache_8,\n",
        "                kv_cache_12=kv_cache_12,\n",
        "                logits=outputs.logits.float()\n",
        "            )\n",
        "\n",
        "        return Lfm2IDKOutput(\n",
        "            logits=outputs.logits,\n",
        "            idk_score=idk_score,\n",
        "            idk_components=idk_components,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "        )\n",
        "\n",
        "    def get_idk_parameters(self):\n",
        "        \"\"\"Get only IDK head parameters for training.\"\"\"\n",
        "        return self.idk_head.parameters()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 6: Training Data Extraction\n",
        "# =============================================================================\n",
        "\n",
        "def extract_samples_from_base_model(base_model, tokenizer, prompts, max_tokens=50):\n",
        "    \"\"\"\n",
        "    Extract (z8, z12) training pairs from the base model.\n",
        "    Uses the base model directly to avoid wrapper issues.\n",
        "    \"\"\"\n",
        "    template = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    samples = []\n",
        "\n",
        "    base_model.eval()\n",
        "    device = next(base_model.parameters()).device\n",
        "\n",
        "    for prompt in tqdm(prompts, desc=\"Extracting samples\"):\n",
        "        input_text = template.format(question=prompt)\n",
        "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        past = None\n",
        "        current = input_ids\n",
        "\n",
        "        for step in range(max_tokens):\n",
        "            with torch.no_grad():\n",
        "                outputs = base_model(\n",
        "                    input_ids=current,\n",
        "                    past_key_values=past,\n",
        "                    use_cache=True,\n",
        "                    output_hidden_states=True,\n",
        "                    return_dict=True\n",
        "                )\n",
        "\n",
        "            # Extract hidden states\n",
        "            # Layer 8 output is at index 9, layer 12 is at index 13\n",
        "            hs = outputs.hidden_states\n",
        "            z8 = hs[9][:, -1, :].float().cpu().squeeze(0)\n",
        "            z12 = hs[13][:, -1, :].float().cpu().squeeze(0)\n",
        "\n",
        "            samples.append({'z8': z8, 'z12': z12})\n",
        "\n",
        "            # Get next token\n",
        "            next_logits = outputs.logits[:, -1, :]\n",
        "            next_token = next_logits.argmax(dim=-1)\n",
        "\n",
        "            # Check EOS\n",
        "            if next_token.item() == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            # Update for next step\n",
        "            past = outputs.past_key_values\n",
        "            current = next_token.unsqueeze(0)\n",
        "\n",
        "    return samples\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 7: Training Function\n",
        "# =============================================================================\n",
        "\n",
        "class SampleDataset(Dataset):\n",
        "    def __init__(self, samples):\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "def train_idk_head(model, samples, epochs=20, lr=1e-4, batch_size=64):\n",
        "    \"\"\"Train the IDK head using flow prediction loss.\"\"\"\n",
        "\n",
        "    dataset = SampleDataset(samples)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    device = next(model.idk_head.parameters()).device\n",
        "    optimizer = torch.optim.AdamW(model.get_idk_parameters(), lr=lr, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    print(f\"Training on {len(samples)} samples, {len(loader)} batches per epoch\")\n",
        "\n",
        "    model.idk_head.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch in loader:\n",
        "            z8 = batch['z8'].to(device)\n",
        "            z12 = batch['z12'].to(device)\n",
        "\n",
        "            # Flow prediction loss\n",
        "            z12_pred = model.idk_head.flow_predictor(z8)\n",
        "            loss = F.mse_loss(z12_pred, z12)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.idk_head.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = total_loss / num_batches\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {avg_loss:.6f}\")\n",
        "\n",
        "    model.idk_head.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 8: RUN THE COMPLETE PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "# --- STEP 1: Load base model and tokenizer ---\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: Loading LFM2 base model...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-1.2B\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"LiquidAI/LFM2-1.2B\",\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"Base model loaded on {base_model.device}\")\n",
        "\n",
        "# --- STEP 2: Create wrapper with IDK head ---\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 2: Creating LFM2 + IDK wrapper...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model = LFM2WithIDK(base_model=base_model)\n",
        "model.to(DEVICE)\n",
        "\n",
        "idk_params = sum(p.numel() for p in model.idk_head.parameters())\n",
        "print(f\"IDK head parameters: {idk_params:,}\")\n",
        "\n",
        "# --- STEP 3: Prepare training prompts ---\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 3: Preparing training prompts...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "training_prompts = [\n",
        "\n",
        "    # =========================================================================\n",
        "    # CATEGORY 1: FACTUAL - COMMON KNOWLEDGE (Model should be CONFIDENT)\n",
        "    # =========================================================================\n",
        "\n",
        "    # Geography\n",
        "    \"What is the capital of France?\",\n",
        "    \"What is the capital of Japan?\",\n",
        "    \"What is the capital of Germany?\",\n",
        "    \"What is the capital of Australia?\",\n",
        "    \"What is the capital of Brazil?\",\n",
        "    \"What is the capital of Canada?\",\n",
        "    \"What is the capital of Egypt?\",\n",
        "    \"What is the capital of India?\",\n",
        "    \"What is the largest country by land area?\",\n",
        "    \"What is the smallest country in the world?\",\n",
        "    \"How many continents are there?\",\n",
        "    \"What is the longest river in the world?\",\n",
        "    \"What is the largest ocean?\",\n",
        "    \"What is the tallest mountain on Earth?\",\n",
        "\n",
        "    # Science basics\n",
        "    \"What is water made of?\",\n",
        "    \"What is the chemical formula for water?\",\n",
        "    \"What is the chemical symbol for gold?\",\n",
        "    \"What is the chemical symbol for oxygen?\",\n",
        "    \"How many planets are in our solar system?\",\n",
        "    \"What is the closest planet to the Sun?\",\n",
        "    \"What is the largest planet in our solar system?\",\n",
        "    \"What is the speed of light?\",\n",
        "    \"What is the boiling point of water in Celsius?\",\n",
        "    \"What is the freezing point of water in Fahrenheit?\",\n",
        "\n",
        "    # History & Literature\n",
        "    \"Who wrote Hamlet?\",\n",
        "    \"Who wrote Romeo and Juliet?\",\n",
        "    \"Who wrote 1984?\",\n",
        "    \"Who wrote Pride and Prejudice?\",\n",
        "    \"Who painted the Mona Lisa?\",\n",
        "    \"Who invented the telephone?\",\n",
        "    \"Who invented the light bulb?\",\n",
        "    \"What year did World War 2 end?\",\n",
        "    \"What year did World War 1 begin?\",\n",
        "    \"Who was the first person to walk on the moon?\",\n",
        "\n",
        "    # Math & Numbers\n",
        "    \"What is 2 + 2?\",\n",
        "    \"What is the square root of 144?\",\n",
        "    \"How many days are in a year?\",\n",
        "    \"How many hours are in a day?\",\n",
        "    \"How many seconds are in a minute?\",\n",
        "    \"What is the value of pi to two decimal places?\",\n",
        "\n",
        "    # =========================================================================\n",
        "    # CATEGORY 2: TECHNICAL - EXPLANATIONS (Model should be moderately confident)\n",
        "    # =========================================================================\n",
        "\n",
        "    \"Explain how neural networks learn\",\n",
        "    \"What is machine learning?\",\n",
        "    \"How does photosynthesis work?\",\n",
        "    \"What is quantum entanglement?\",\n",
        "    \"What is the difference between TCP and UDP?\",\n",
        "    \"Explain the theory of relativity\",\n",
        "    \"What is CRISPR gene editing?\",\n",
        "    \"How do vaccines work?\",\n",
        "    \"What is blockchain technology?\",\n",
        "    \"How does encryption work?\",\n",
        "    \"What is an algorithm?\",\n",
        "    \"Explain how a CPU processes instructions\",\n",
        "    \"What is the difference between RAM and ROM?\",\n",
        "    \"How does GPS work?\",\n",
        "    \"What is artificial intelligence?\",\n",
        "    \"Explain the water cycle\",\n",
        "    \"How do airplanes fly?\",\n",
        "    \"What causes earthquakes?\",\n",
        "    \"How does the immune system work?\",\n",
        "    \"What is DNA and how does it work?\",\n",
        "    \"Explain how batteries store energy\",\n",
        "    \"What is the difference between AC and DC current?\",\n",
        "\n",
        "    # =========================================================================\n",
        "    # CATEGORY 3: RECENT/TEMPORAL (Model should be UNCERTAIN - data cutoff)\n",
        "    # =========================================================================\n",
        "\n",
        "    \"Who won the most recent Super Bowl?\",\n",
        "    \"What is the current stock price of Apple?\",\n",
        "    \"Who is the current president of France?\",\n",
        "    \"Who is the current CEO of Twitter?\",\n",
        "    \"What was the weather yesterday in New York?\",\n",
        "    \"What movies came out this week?\",\n",
        "    \"What is the current price of Bitcoin?\",\n",
        "    \"Who won the most recent Grammy for Album of the Year?\",\n",
        "    \"What is the current population of the world?\",\n",
        "    \"Who won the last FIFA World Cup?\",\n",
        "    \"What is the current interest rate set by the Federal Reserve?\",\n",
        "    \"What was the closing price of the S&P 500 yesterday?\",\n",
        "\n",
        "    # =========================================================================\n",
        "    # CATEGORY 4: SUBJECTIVE/OPINION (Model should be UNCERTAIN)\n",
        "    # =========================================================================\n",
        "\n",
        "    \"What is the best programming language?\",\n",
        "    \"Is pineapple good on pizza?\",\n",
        "    \"What is the meaning of life?\",\n",
        "    \"What is the best movie ever made?\",\n",
        "    \"Should I learn Python or JavaScript?\",\n",
        "    \"What is the best country to live in?\",\n",
        "    \"Is coffee better than tea?\",\n",
        "    \"What is the best music genre?\",\n",
        "    \"Should I buy a Mac or PC?\",\n",
        "    \"What is the most beautiful city in the world?\",\n",
        "    \"Is it better to rent or buy a house?\",\n",
        "    \"What is the best way to learn a new language?\",\n",
        "    \"Should I exercise in the morning or evening?\",\n",
        "    \"What is the best career choice for someone who likes math?\",\n",
        "    \"Is reading better than watching movies?\",\n",
        "\n",
        "    # =========================================================================\n",
        "    # CATEGORY 5: IMPOSSIBLE - FUTURE PREDICTIONS (Model should be VERY UNCERTAIN)\n",
        "    # =========================================================================\n",
        "\n",
        "    \"What will happen tomorrow?\",\n",
        "    \"Who will win the next election?\",\n",
        "    \"What will the stock market do next week?\",\n",
        "    \"Will it rain on my birthday?\",\n",
        "    \"What will AI look like in 100 years?\",\n",
        "    \"Who will win the next World Cup?\",\n",
        "    \"What will be the next major scientific discovery?\",\n",
        "    \"When will humans land on Mars?\",\n",
        "    \"What will be the biggest news story next month?\",\n",
        "    \"Will there be a recession next year?\",\n",
        "    \"What company will be the most valuable in 10 years?\",\n",
        "    \"What will be the next pandemic?\",\n",
        "\n",
        "    # =========================================================================\n",
        "    # CATEGORY 6: IMPOSSIBLE - PERSONAL/PRIVATE (Model CANNOT know)\n",
        "    # User-provided questions about personal unknowable information\n",
        "    # =========================================================================\n",
        "\n",
        "    \"What will I personally eat for dinner tomorrow night?\",\n",
        "    \"What number am I thinking of right now?\",\n",
        "    \"What will be the first typo I make tomorrow?\",\n",
        "    \"What is the password to my personal email account?\",\n",
        "    \"What will be the exact time I fall asleep tonight?\",\n",
        "    \"What decision will I make five minutes from now?\",\n",
        "    \"What is written on the last sticky note I used?\",\n",
        "    \"What will the weather be at my exact location in 30 days?\",\n",
        "    \"What song will get stuck in my head next?\",\n",
        "    \"What is inside the unopened box on my desk?\",\n",
        "    \"What will be the next thought I have?\",\n",
        "    \"What will be the next object I touch?\",\n",
        "    \"What will my mood be exactly at noon tomorrow?\",\n",
        "    \"What private message did I last delete?\",\n",
        "    \"What will be the exact price of gas at my nearest station tomorrow?\",\n",
        "    \"What will be the next dream I remember?\",\n",
        "    \"What will I write as my next password?\",\n",
        "    \"What will be the next mistake I make today?\",\n",
        "    \"What will be the exact wording of my next text message?\",\n",
        "    \"What will be the last thing I think about before I fall asleep tonight?\",\n",
        "\n",
        "    # =========================================================================\n",
        "    # CATEGORY 7: OBSCURE FACTUAL (Model likely UNCERTAIN - rare knowledge)\n",
        "    # User-provided questions about obscure facts\n",
        "    # =========================================================================\n",
        "\n",
        "    \"Who received the IEEE Frank Rosenblatt Award in 2010?\",\n",
        "    \"Who was awarded the Oceanography Society's Jerlov Award in 2018?\",\n",
        "    \"What's the name of the women's liberal arts college in Cambridge, Massachusetts?\",\n",
        "    \"In whose honor was the Leipzig 1877 tournament organized?\",\n",
        "    \"According to Karl Küchler, what did Empress Elizabeth of Austria's favorite sculpture depict, which was made for her villa Achilleion at Corfu?\",\n",
        "    \"How much money, in euros, was the surgeon held responsible for Stella Obasanjo's death ordered to pay her son?\",\n",
        "    \"What were the month and year when Obama told Christianity Today, 'I am a Christian, and I am a devout Christian. I believe in the redemptive death and resurrection of Jesus Christ'?\",\n",
        "    \"Who appointed the Chief Justice of India, Mirza Hameedullah Beg, in 1977?\",\n",
        "    \"What is the name of the former Prime Minister of Iceland who worked as a cabin crew member until 1971?\",\n",
        "    \"To whom did Mehbooba Mufti Sayed contest the 2019 Lok Sabha elections and lose?\",\n",
        "    \"How many fouls did Inter commit in the Champions League final match between Bayern and Inter on May 23, 2010?\",\n",
        "    \"What year did the Lego part with ID gal56 first release?\",\n",
        "    \"In which year did the Japanese scientist Koichi Mizushima receive the Kato Memorial Prize?\",\n",
        "    \"In which year did Melbourne's Monash Gallery of Art (MGA) rebrand and become the Museum of Australian Photography (MAPh)?\",\n",
        "    \"Who requested the Federal Aviation Administration (FAA) implement a 900 sq mi (2,300 km²) temporary flight restriction zone over the operations areas of the Deepwater Horizon?\",\n",
        "    \"What signature piece of the MOBA did Scott Wilson discover on the curb between two trash cans?\",\n",
        "    \"What player scored all the conversions for Spain in the rugby match between Spain and Romania that was part of the 2022 Rugby Europe Championship on February 27, 2022?\",\n",
        "    \"What is the surname of the psychiatrist who prescribes medication for Marie Hanson for her periodic blackouts in Season 1, Episode 20 of Ally McBeal?\",\n",
        "    \"What is the British-American kickboxer Andrew Tate's kickboxing name?\",\n",
        "    \"What position was John Gilbert Layton appointed to in Quebec from 1969 until 1970?\",\n",
        "\n",
        "    # =========================================================================\n",
        "    # CATEGORY 8: ADDITIONAL OBSCURE/TRIVIA (Model likely UNCERTAIN)\n",
        "    # =========================================================================\n",
        "\n",
        "    \"What is the population of Tuvalu?\",\n",
        "    \"Who was the 23rd Prime Minister of Canada?\",\n",
        "    \"What is the atomic weight of Lawrencium?\",\n",
        "    \"What is the GDP of Liechtenstein?\",\n",
        "    \"Who won the Nobel Prize in Chemistry in 1987?\",\n",
        "    \"What is the deepest point in the Indian Ocean?\",\n",
        "    \"Who was the Roman Emperor in 117 AD?\",\n",
        "    \"What is the national bird of Bhutan?\",\n",
        "    \"Who designed the Sydney Opera House?\",\n",
        "    \"What year was the Treaty of Westphalia signed?\",\n",
        "    \"What is the chemical formula for rust?\",\n",
        "    \"Who invented the zipper?\",\n",
        "    \"What is the currency of Myanmar?\",\n",
        "    \"What is the second largest moon of Saturn?\",\n",
        "    \"Who wrote the novel 'The Master and Margarita'?\",\n",
        "    \"What year was the first email sent?\",\n",
        "    \"What is the tallest building in South America?\",\n",
        "    \"Who was the first female Prime Minister of Pakistan?\",\n",
        "    \"What is the capital of Burkina Faso?\",\n",
        "    \"What element has the atomic number 79?\",\n",
        "\n",
        "    # =========================================================================\n",
        "    # CATEGORY 9: ADDITIONAL IMPOSSIBLE/RANDOM (Model should be VERY UNCERTAIN)\n",
        "    # =========================================================================\n",
        "\n",
        "    \"What am I thinking right now?\",\n",
        "    \"What will be the lottery numbers next week?\",\n",
        "    \"What is the exact number of grains of sand on Earth?\",\n",
        "    \"What will be the exact temperature at noon tomorrow in my backyard?\",\n",
        "    \"How many birds are flying right now?\",\n",
        "    \"What will be the next word I type?\",\n",
        "    \"What color socks am I wearing?\",\n",
        "    \"What did I have for breakfast three weeks ago?\",\n",
        "    \"What will be trending on social media tomorrow?\",\n",
        "    \"What is my favorite color?\",\n",
        "    \"How many times will I blink today?\",\n",
        "    \"What will be my next Google search?\",\n",
        "    \"What will I name my future pet?\",\n",
        "    \"What time will I wake up next Saturday?\",\n",
        "    \"What will be the headline news in exactly one year?\",\n",
        "\n",
        "    # =========================================================================\n",
        "    # CATEGORY 10: PARADOXES AND UNANSWERABLE (Model should be VERY UNCERTAIN)\n",
        "    # =========================================================================\n",
        "\n",
        "    \"What happens when an unstoppable force meets an immovable object?\",\n",
        "    \"Can God create a rock so heavy that even God cannot lift it?\",\n",
        "    \"If a tree falls in a forest and no one is around, does it make a sound?\",\n",
        "    \"What existed before the universe?\",\n",
        "    \"What is north of the North Pole?\",\n",
        "    \"What is the sound of one hand clapping?\",\n",
        "    \"Why is there something rather than nothing?\",\n",
        "    \"What is outside the universe?\",\n",
        "    \"Can you prove that reality is not a simulation?\",\n",
        "    \"What is the last digit of pi?\",\n",
        "]\n",
        "\n",
        "print(f\"Prepared {len(training_prompts)} diverse prompts\")\n",
        "\n",
        "# --- STEP 4: Extract training samples ---\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 4: Extracting training samples...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "samples = extract_samples_from_base_model(\n",
        "    base_model=base_model,  # Use base model directly\n",
        "    tokenizer=tokenizer,\n",
        "    prompts=training_prompts,\n",
        "    max_tokens=30\n",
        ")\n",
        "\n",
        "print(f\"Extracted {len(samples)} (z8, z12) pairs\")\n",
        "\n",
        "# --- STEP 5: Train IDK head ---\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 5: Training IDK head...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model = train_idk_head(model, samples, epochs=15, lr=1e-4, batch_size=32)\n",
        "\n",
        "# --- STEP 6: Test inference ---\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 6: Testing inference with IDK scores...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_prompts = [\n",
        "    (\"What is 2 + 2?\", \"Should be CONFIDENT\"),\n",
        "    (\"What is the capital of France?\", \"Should be CONFIDENT\"),\n",
        "    (\"What is the meaning of life?\", \"Should be UNCERTAIN\"),\n",
        "    (\"Who will win the lottery tomorrow?\", \"Should be VERY UNCERTAIN\"),\n",
        "    (\"What is the 1000th digit of pi?\", \"Should be VERY UNCERTAIN\"),\n",
        "]\n",
        "\n",
        "template = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "model.eval()\n",
        "print(\"\\nResults:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for question, expected in test_prompts:\n",
        "    input_text = template.format(question=question)\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, output_idk_score=True)\n",
        "\n",
        "    idk = outputs.idk_score\n",
        "    if isinstance(idk, torch.Tensor):\n",
        "        idk = idk.item() if idk.numel() == 1 else idk[0].item()\n",
        "\n",
        "    # Interpret\n",
        "    if idk < 20:\n",
        "        conf = \"🟢 Very Confident\"\n",
        "    elif idk < 40:\n",
        "        conf = \"🟡 Confident\"\n",
        "    elif idk < 60:\n",
        "        conf = \"🟠 Uncertain\"\n",
        "    elif idk < 80:\n",
        "        conf = \"🔴 Low Confidence\"\n",
        "    else:\n",
        "        conf = \"⚫ Very Uncertain\"\n",
        "\n",
        "    print(f\"\\nQ: {question}\")\n",
        "    print(f\"   IDK Score: {idk:.1f}/100 {conf}\")\n",
        "    print(f\"   Expected: {expected}\")\n",
        "    if outputs.idk_components:\n",
        "        print(f\"   Components: {outputs.idk_components}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CELL 9: (OPTIONAL) Generate text with per-token IDK\n",
        "# =============================================================================\n",
        "\n",
        "def generate_with_idk(model, tokenizer, prompt, max_tokens=30, temperature=0.3):\n",
        "    \"\"\"Generate text with per-token IDK scores.\"\"\"\n",
        "    template = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "    input_text = template.format(question=prompt)\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    generated_tokens = []\n",
        "    idk_scores = []\n",
        "\n",
        "    current = input_ids\n",
        "    past = None\n",
        "\n",
        "    model.eval()\n",
        "    for _ in range(max_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(current, past_key_values=past, output_idk_score=True)\n",
        "\n",
        "        # Get IDK score\n",
        "        idk = outputs.idk_score\n",
        "        if isinstance(idk, torch.Tensor):\n",
        "            idk = idk.item() if idk.numel() == 1 else idk[0].item()\n",
        "        idk_scores.append(idk)\n",
        "\n",
        "        # Sample next token\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "        if temperature > 0:\n",
        "            probs = F.softmax(logits / temperature, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            next_token = logits.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "        generated_tokens.append(next_token.item())\n",
        "\n",
        "        # Check EOS\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        past = outputs.past_key_values\n",
        "        current = next_token\n",
        "\n",
        "    text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'tokens': generated_tokens,\n",
        "        'idk_scores': idk_scores,\n",
        "        'mean_idk': sum(idk_scores) / len(idk_scores) if idk_scores else 0,\n",
        "    }\n",
        "\n",
        "\n",
        "# Test generation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BONUS: Generation with per-token IDK\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = generate_with_idk(model, tokenizer, \"What is machine learning?\", max_tokens=20)\n",
        "print(f\"\\nQuestion: What is machine learning?\")\n",
        "print(f\"Answer: {result['text']}\")\n",
        "print(f\"Mean IDK: {result['mean_idk']:.1f}/100\")\n",
        "print(f\"\\nPer-token IDK scores:\")\n",
        "for i, (tok, idk) in enumerate(zip(result['tokens'], result['idk_scores'])):\n",
        "    tok_str = tokenizer.decode([tok])\n",
        "    print(f\"  {i}: '{tok_str}' -> IDK: {idk:.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "973e89caff714145bee3de3237e7abb7",
            "70e1d607c07f416f997a435461b254df",
            "6e07a7164d3848d7a7ca52df8f3ec09f",
            "3709fb5914bd4344a433d8c33ccb927f",
            "d5dad51295f9474397001862a4ed862c",
            "904c690d24c3480195213a422ac9573d",
            "8c9fec93e4cb462fa32372ec26484740",
            "7a4bbf38f05c46f2b981ad898fda4363",
            "cfc424fba3d2459c8f41de6cffcf4b13",
            "d24153e1235248bf959410ca1a531c6c",
            "c659f57965a345d1acdd57d1cca822e1",
            "52fb4bbba94c499588ffb8fa5ba9460d",
            "94e6f9d0f8c64732bcb413b5c0843d1c",
            "22c8288115224fdda8ebaa0eac50f068",
            "d374d126503a4a3abb061a9e5855f7ec",
            "98878be7b7ea48c1807eead441a1c250",
            "6f7ef74734c74f189078fe010d20d29e",
            "c7e03561c7244b47afbd7bf80482b764",
            "ecdef0ddf3a74550a208570c15eff343",
            "0969f12310854a709b4b555fe50b9499",
            "4b2057361e0045a287f3980cd2e71eb0",
            "98bdec6e10844a74b8682685a388af3a",
            "c1c047886af44406b5aa320a2f5b70d9",
            "7048187a9d4a47728de7a48d4f41a53c",
            "e876fc68d7c04a07892057ae2e8815aa",
            "20b2db4981f94bfc86f8c12f785edabe",
            "ace476d3ae944ccfb0419aae9595f048",
            "e152ff46cb0e4d7a8b07c7c4ef2f5eb0",
            "7f897b402ba44013847597b865b91f0e",
            "efef9d7afebd4a22bd94fd19408565a6",
            "0c70110a6f874a7796b5d692cf06ccfe",
            "7295505377ae4ace95484d872cb9bd0a",
            "fe91d15f2ede470098158b52897a4cb7",
            "8b83598bbca54caf9d88d6b36a3b92f8",
            "afdf5f5a66274f16a8251e9b47bbf28b",
            "59e5116d4e894695bf15054df4667ee2",
            "af8f0aa77b904d4ead41bcfbd5c22466",
            "9848674edb9d459aae3e988b043e566e",
            "a57894a481584922aa5457e3ed624979",
            "68e432edf76c4fd7b091026cd748005e",
            "e36cf92d0e894ba1b2fe61f8a13caf02",
            "673e35666fb44d9088c1246997fa8bd1",
            "3464fe74ab8b4aa7b0c1e761c1632bef",
            "c5158f8f97604fbdb45ffe3b920a3ba6",
            "c235ba61d2b54a3a98879253f572f74a",
            "5fae5feb31eb4e0b92264a92ed15da63",
            "eec6f22b267648909855c669817f7cd2",
            "a9fb171a06a34867be942803db964973",
            "67db54a07d6346a4858bc1fe107ddf63",
            "a7051f65e2f943c2830d2e5ab497067f",
            "7497d28b47a44f44a2b38b62fcc7fa56",
            "fddaab06a30649ac84bc737496faed82",
            "0636fb3a593e4275b68703f236d62525",
            "4f7b4b4e47a04d79b9a5ae3df1f47ab5",
            "f3c23b13c02b4f939936619df0ffebbf",
            "a3720672ffec468da5fff92d4fdba22f",
            "5b1546a15e6c4e0ea44b91506f397703",
            "6dfea70f95044878a81ecc159547de0c",
            "8e1aa11f3a7c4a1e914e488f0542ab21",
            "54a8e9c7ced543f6b08100b9881ebf8e",
            "bbe7afbebe684807a0f5ca43c390886e",
            "8a3fbc04cf454417b888119513b3722e",
            "97cc8de86809447ea002bcbcd9092265",
            "6aa061768fcf4bd89810fd218edc1a04",
            "a3d287d2dfed4bdfac977d1c50b021d7",
            "e3b95eaf4a304da9b4bec80aa6c4be46",
            "970b521546104d379d552f8c999ab8c8",
            "1975897c693344869e506d24542bda64",
            "27fdd93f02c449ab8c27de1dbb93d08c",
            "cfbaa5b8fbf348368701a3bf5f9b20b8",
            "1c7ed2748e544c9f98a438b881c1395e",
            "48959ce0f12e45eb8733121294e05480",
            "f099fda6d70040e4ba7a2abc03cc0ded",
            "95900df09dc34e10b5f9aef88833c789",
            "5c24b062860c44dc85254bc0493c53a0",
            "9bc1a5fb5b8b4f4c8da216d51e310b2a",
            "45a0116e3e234eb98d7d6d55533550ef"
          ]
        },
        "id": "aTweSmN9lGls",
        "outputId": "d2d4b20d-c55d-4929-d1a9-17999fd42076"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "============================================================\n",
            "STEP 1: Loading LFM2 base model...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "973e89caff714145bee3de3237e7abb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52fb4bbba94c499588ffb8fa5ba9460d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1c047886af44406b5aa320a2f5b70d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8b83598bbca54caf9d88d6b36a3b92f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c235ba61d2b54a3a98879253f572f74a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.34G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3720672ffec468da5fff92d4fdba22f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "970b521546104d379d552f8c999ab8c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model loaded on cuda:0\n",
            "\n",
            "============================================================\n",
            "STEP 2: Creating LFM2 + IDK wrapper...\n",
            "============================================================\n",
            "IDK head parameters: 2,364,422\n",
            "\n",
            "============================================================\n",
            "STEP 3: Preparing training prompts...\n",
            "============================================================\n",
            "Prepared 186 diverse prompts\n",
            "\n",
            "============================================================\n",
            "STEP 4: Extracting training samples...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting samples: 100%|██████████| 186/186 [01:40<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 5344 (z8, z12) pairs\n",
            "\n",
            "============================================================\n",
            "STEP 5: Training IDK head...\n",
            "============================================================\n",
            "Training on 5344 samples, 167 batches per epoch\n",
            "Epoch   1/15 | Loss: 0.115839\n",
            "Epoch   5/15 | Loss: 0.051918\n",
            "Epoch  10/15 | Loss: 0.040745\n",
            "Epoch  15/15 | Loss: 0.039628\n",
            "\n",
            "============================================================\n",
            "STEP 6: Testing inference with IDK scores...\n",
            "============================================================\n",
            "\n",
            "Results:\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Q: What is 2 + 2?\n",
            "   IDK Score: 63.6/100 🔴 Low Confidence\n",
            "   Expected: Should be CONFIDENT\n",
            "   Components: {'flow_error': 0.5061379671096802, 'head_disagreement': 0.9274423718452454, 'entropy_signal': 0.5170724391937256}\n",
            "\n",
            "Q: What is the capital of France?\n",
            "   IDK Score: 63.2/100 🔴 Low Confidence\n",
            "   Expected: Should be CONFIDENT\n",
            "   Components: {'flow_error': 0.5049847960472107, 'head_disagreement': 0.9281729459762573, 'entropy_signal': 0.5042762756347656}\n",
            "\n",
            "Q: What is the meaning of life?\n",
            "   IDK Score: 63.1/100 🔴 Low Confidence\n",
            "   Expected: Should be UNCERTAIN\n",
            "   Components: {'flow_error': 0.5069972276687622, 'head_disagreement': 0.9245220422744751, 'entropy_signal': 0.5011587142944336}\n",
            "\n",
            "Q: Who will win the lottery tomorrow?\n",
            "   IDK Score: 64.7/100 🔴 Low Confidence\n",
            "   Expected: Should be VERY UNCERTAIN\n",
            "   Components: {'flow_error': 0.5047721266746521, 'head_disagreement': 0.9282389879226685, 'entropy_signal': 0.556220531463623}\n",
            "\n",
            "Q: What is the 1000th digit of pi?\n",
            "   IDK Score: 64.4/100 🔴 Low Confidence\n",
            "   Expected: Should be VERY UNCERTAIN\n",
            "   Components: {'flow_error': 0.5076509118080139, 'head_disagreement': 0.9231809377670288, 'entropy_signal': 0.5455262660980225}\n",
            "\n",
            "============================================================\n",
            "✅ COMPLETE!\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "BONUS: Generation with per-token IDK\n",
            "============================================================\n",
            "\n",
            "Question: What is machine learning?\n",
            "Answer: Machine learning is a subset of artificial intelligence (AI) that enables computers to learn from and make decisions\n",
            "Mean IDK: 63.5/100\n",
            "\n",
            "Per-token IDK scores:\n",
            "  0: 'Machine' -> IDK: 63.1\n",
            "  1: ' learning' -> IDK: 63.3\n",
            "  2: ' is' -> IDK: 63.7\n",
            "  3: ' a' -> IDK: 63.4\n",
            "  4: ' subset' -> IDK: 63.9\n",
            "  5: ' of' -> IDK: 63.2\n",
            "  6: ' artificial' -> IDK: 63.0\n",
            "  7: ' intelligence' -> IDK: 63.1\n",
            "  8: ' (' -> IDK: 63.8\n",
            "  9: 'AI' -> IDK: 63.1\n",
            "  10: ')' -> IDK: 63.3\n",
            "  11: ' that' -> IDK: 63.8\n",
            "  12: ' enables' -> IDK: 64.2\n",
            "  13: ' computers' -> IDK: 64.0\n",
            "  14: ' to' -> IDK: 63.2\n",
            "  15: ' learn' -> IDK: 63.2\n",
            "  16: ' from' -> IDK: 63.7\n",
            "  17: ' and' -> IDK: 63.7\n",
            "  18: ' make' -> IDK: 62.9\n",
            "  19: ' decisions' -> IDK: 63.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save the model weights for HuggingFace"
      ],
      "metadata": {
        "id": "5o-q69-zt-Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "torch.save(model.idk_head.state_dict(), \"pytorch_model.bin\")\n",
        "files.download(\"pytorch_model.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "khpTmGCUlOTM",
        "outputId": "53d60e25-421c-4789-9e20-49b2765a886c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0cf1899c-8c4b-4a4f-8f57-9abc0e5134ba\", \"pytorch_model.bin\", 9464247)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo the Liquid AI model and Bullshit Detector in action"
      ],
      "metadata": {
        "id": "Ni4rOVNtuH85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# DEMO CELL - Run after training completes\n",
        "# =============================================================================\n",
        "# The model and tokenizer are already loaded from training above.\n",
        "# Just change QUESTION and re-run this cell to test different prompts!\n",
        "# =============================================================================\n",
        "\n",
        "# ============================================\n",
        "# CHANGE THIS QUESTION AND RE-RUN!\n",
        "# ============================================\n",
        "\n",
        "QUESTION = \"What is the capital of France?\"\n",
        "\n",
        "# ============================================\n",
        "\n",
        "# Template for LFM2\n",
        "template = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "# Encode\n",
        "prompt = template.format(question=QUESTION)\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "# Generate with IDK tracking\n",
        "generated_tokens = []\n",
        "idk_scores = []\n",
        "current = input_ids\n",
        "past = None\n",
        "\n",
        "model.eval()\n",
        "for _ in range(50):  # max tokens\n",
        "    with torch.no_grad():\n",
        "        outputs = model(current, past_key_values=past, output_idk_score=True)\n",
        "\n",
        "    # Get IDK score\n",
        "    idk = outputs.idk_score\n",
        "    if isinstance(idk, torch.Tensor):\n",
        "        idk = idk.item() if idk.numel() == 1 else idk[0].item()\n",
        "    idk_scores.append(idk)\n",
        "\n",
        "    # Sample next token\n",
        "    logits = outputs.logits[:, -1, :]\n",
        "    probs = F.softmax(logits / 0.3, dim=-1)  # temperature=0.3\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    generated_tokens.append(next_token.item())\n",
        "\n",
        "    # Check EOS\n",
        "    if next_token.item() == tokenizer.eos_token_id:\n",
        "        break\n",
        "\n",
        "    past = outputs.past_key_values\n",
        "    current = next_token\n",
        "\n",
        "# Decode answer\n",
        "answer = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "mean_idk = sum(idk_scores) / len(idk_scores) if idk_scores else 0\n",
        "\n",
        "# Get label\n",
        "if mean_idk < 30:\n",
        "    label = \"[GREEN] Confident\"\n",
        "elif mean_idk < 50:\n",
        "    label = \"[YELLOW] Moderate\"\n",
        "elif mean_idk < 70:\n",
        "    label = \"[ORANGE] Uncertain\"\n",
        "else:\n",
        "    label = \"[RED] Very Uncertain\"\n",
        "\n",
        "# Display results\n",
        "print(\"=\" * 60)\n",
        "print(\"QUESTION:\", QUESTION)\n",
        "print(\"-\" * 60)\n",
        "print(\"ANSWER:\", answer)\n",
        "print(\"-\" * 60)\n",
        "print(\"IDK SCORE: {:.1f}/100 - {}\".format(mean_idk, label))\n",
        "print(\"=\" * 60)\n",
        "print(\"\")\n",
        "print(\"IDK Score Guide:\")\n",
        "print(\"  0-30  = Confident (likely reliable)\")\n",
        "print(\"  30-50 = Moderate (probably fine)\")\n",
        "print(\"  50-70 = Uncertain (verify this)\")\n",
        "print(\"  70-100 = Very Uncertain (high hallucination risk)\")"
      ],
      "metadata": {
        "id": "EJoesx-XrkCw",
        "outputId": "17acc60d-ac4e-4ab2-e6fa-55184ecde523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "QUESTION: What is the capital of France?\n",
            "------------------------------------------------------------\n",
            "ANSWER: The capital of France is Paris. This city is not only the political center of France, but also a global hub for art, fashion, gastronomy, and culture. Known as the \"City of Light\" or \"La Ville Lumière,\"\n",
            "------------------------------------------------------------\n",
            "IDK SCORE: 63.4/100 - [ORANGE] Uncertain\n",
            "============================================================\n",
            "\n",
            "IDK Score Guide:\n",
            "  0-30  = Confident (likely reliable)\n",
            "  30-50 = Moderate (probably fine)\n",
            "  50-70 = Uncertain (verify this)\n",
            "  70-100 = Very Uncertain (high hallucination risk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwZMq-wtrj8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lion_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uai1N9PTuvqy",
        "outputId": "b2f1ef80-9665-4c7b-9a93-03ce4eb24a6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lion_pytorch\n",
            "  Downloading lion_pytorch-0.2.3-py3-none-any.whl.metadata (616 bytes)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.12/dist-packages (from lion_pytorch) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6->lion_pytorch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6->lion_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6->lion_pytorch) (3.0.3)\n",
            "Downloading lion_pytorch-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Installing collected packages: lion_pytorch\n",
            "Successfully installed lion_pytorch-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TEMPLATE_WITHOUT_ANSWER = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ],
      "metadata": {
        "id": "deLZFMLBX4nc",
        "outputId": "50edbd0f-ef1f-4d1b-d902-6dbf12d01292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class HallucinationFeatures:\n",
        "    \"\"\"Features extracted for hallucination detection.\"\"\"\n",
        "    prompt: str\n",
        "    question: str\n",
        "    input_length: int\n",
        "    z8_states: List[torch.Tensor] = field(default_factory=list)\n",
        "    z12_states: List[torch.Tensor] = field(default_factory=list)\n",
        "    kv8_head_disagreement: List[float] = field(default_factory=list)\n",
        "    kv12_head_disagreement: List[float] = field(default_factory=list)\n",
        "    generated_tokens: List[int] = field(default_factory=list)\n",
        "    token_probs: List[float] = field(default_factory=list)\n",
        "    token_entropies: List[float] = field(default_factory=list)\n",
        "    top5_tokens: List[List[Tuple[str, float]]] = field(default_factory=list)\n",
        "    full_text: str = \"\"\n",
        "    mean_prob: float = 0.0\n",
        "    mean_entropy: float = 0.0\n",
        "    mean_head_disagreement_8: float = 0.0\n",
        "    mean_head_disagreement_12: float = 0.0"
      ],
      "metadata": {
        "id": "Azidyx3IX4lm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_head_disagreement(kv_cache_layer) -> float:\n",
        "    \"\"\"Compute variance across KV heads - higher = more disagreement.\"\"\"\n",
        "    k, v = kv_cache_layer\n",
        "    if k.numel() == 0:\n",
        "        return 0.0\n",
        "    k_last = k[:, :, -1, :].float()\n",
        "    v_last = v[:, :, -1, :].float()\n",
        "    return k_last.var(dim=1).mean().item() + v_last.var(dim=1).mean().item()\n",
        "\n",
        "\n",
        "def extract_hallucination_features(model, tokenizer, question: str, max_new_tokens: int = 50) -> HallucinationFeatures:\n",
        "    \"\"\"Extract z8, z12, KV cache stats, and output probabilities.\"\"\"\n",
        "    prompt = TEMPLATE_WITHOUT_ANSWER.format(question=question)\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    features = HallucinationFeatures(prompt=prompt, question=question, input_length=input_ids.shape[1])\n",
        "    past_key_values = None\n",
        "    current_ids = input_ids\n",
        "\n",
        "    for step in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                current_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "                output_hidden_states=True,\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "        # Hidden states: index 9 = layer 8 output, index 13 = layer 12 output\n",
        "        hidden_states = outputs.hidden_states\n",
        "        z8 = hidden_states[9][:, -1, :].float().cpu()\n",
        "        z12 = hidden_states[13][:, -1, :].float().cpu()\n",
        "\n",
        "        features.z8_states.append(z8)\n",
        "        features.z12_states.append(z12)\n",
        "\n",
        "        # KV cache head disagreement\n",
        "        cache = outputs.past_key_values\n",
        "        features.kv8_head_disagreement.append(compute_head_disagreement(cache[8]))\n",
        "        features.kv12_head_disagreement.append(compute_head_disagreement(cache[12]))\n",
        "\n",
        "        # Logits and probabilities\n",
        "        logits = outputs.logits[:, -1, :].float()\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        entropy = -(probs * torch.log(probs + 1e-10)).sum().item()\n",
        "        next_token = torch.argmax(logits, dim=-1)\n",
        "        selected_prob = probs[0, next_token.item()].item()\n",
        "\n",
        "        # Top-5\n",
        "        top5_probs, top5_idx = torch.topk(probs[0], 5)\n",
        "        top5 = [(tokenizer.decode([idx.item()]), prob.item()) for idx, prob in zip(top5_idx, top5_probs)]\n",
        "\n",
        "        features.generated_tokens.append(next_token.item())\n",
        "        features.token_probs.append(selected_prob)\n",
        "        features.token_entropies.append(entropy)\n",
        "        features.top5_tokens.append(top5)\n",
        "\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        past_key_values = outputs.past_key_values\n",
        "        current_ids = next_token.unsqueeze(0)\n",
        "\n",
        "    # Summary stats\n",
        "    features.full_text = tokenizer.decode(features.generated_tokens, skip_special_tokens=True)\n",
        "    features.mean_prob = np.mean(features.token_probs) if features.token_probs else 0.0\n",
        "    features.mean_entropy = np.mean(features.token_entropies) if features.token_entropies else 0.0\n",
        "    features.mean_head_disagreement_8 = np.mean(features.kv8_head_disagreement) if features.kv8_head_disagreement else 0.0\n",
        "    features.mean_head_disagreement_12 = np.mean(features.kv12_head_disagreement) if features.kv12_head_disagreement else 0.0\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "hcIPHWwkX4jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Z8Z12LogitsPredictor(nn.Module):\n",
        "    \"\"\"Predict z12 from z8, and logits summary from z12.\"\"\"\n",
        "    def __init__(self, hidden_dim: int = 2048, bottleneck_dim: int = 512):\n",
        "        super().__init__()\n",
        "        self.z8_to_z12 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(bottleneck_dim),\n",
        "            nn.Linear(bottleneck_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(bottleneck_dim),\n",
        "            nn.Linear(bottleneck_dim, hidden_dim),\n",
        "        )\n",
        "        self.z12_to_logits_summary = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(bottleneck_dim),\n",
        "            nn.Linear(bottleneck_dim, 6),  # entropy + top5 probs\n",
        "        )\n",
        "\n",
        "    def forward(self, z8):\n",
        "        z12_pred = self.z8_to_z12(z8)\n",
        "        logits_summary_pred = self.z12_to_logits_summary(z12_pred)\n",
        "        return z12_pred, logits_summary_pred\n",
        "\n",
        "    def compute_oos_score(self, z8, z12, logits_summary, alpha=0.7, beta=0.3):\n",
        "        z12_pred, logits_pred = self.forward(z8)\n",
        "        err_z12 = F.mse_loss(z12_pred, z12, reduction='none').mean(dim=-1)\n",
        "        err_logits = F.mse_loss(logits_pred, logits_summary, reduction='none').mean(dim=-1)\n",
        "        return alpha * err_z12 + beta * err_logits\n",
        "\n",
        "\n",
        "class HallucinationDataset(Dataset):\n",
        "    def __init__(self, features_list: List[HallucinationFeatures]):\n",
        "        self.samples = []\n",
        "        for feat in features_list:\n",
        "            for i in range(len(feat.z8_states)):\n",
        "                top5_probs = [p for _, p in feat.top5_tokens[i]]\n",
        "                while len(top5_probs) < 5:\n",
        "                    top5_probs.append(0.0)\n",
        "                logits_summary = torch.tensor([feat.token_entropies[i]] + top5_probs[:5], dtype=torch.float32)\n",
        "                self.samples.append({\n",
        "                    'z8': feat.z8_states[i].squeeze(0),\n",
        "                    'z12': feat.z12_states[i].squeeze(0),\n",
        "                    'logits_summary': logits_summary,\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "def train_predictor(features_list, epochs=20, lr=1e-4, batch_size=64):\n",
        "    dataset = HallucinationDataset(features_list)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    print(f\"Training on {len(dataset)} token samples\")\n",
        "\n",
        "    predictor = Z8Z12LogitsPredictor().to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(predictor.parameters(), lr=lr)\n",
        "\n",
        "    predictor.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in loader:\n",
        "            z8 = batch['z8'].to(DEVICE)\n",
        "            z12 = batch['z12'].to(DEVICE)\n",
        "            logits_summary = batch['logits_summary'].to(DEVICE)\n",
        "\n",
        "            z12_pred, logits_pred = predictor(z8)\n",
        "            loss_z12 = F.mse_loss(z12_pred, z12)\n",
        "            loss_logits = F.mse_loss(logits_pred, logits_summary)\n",
        "            loss = 0.7 * loss_z12 + 0.3 * loss_logits\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {total_loss/len(loader):.6f}\")\n",
        "\n",
        "    return predictor"
      ],
      "metadata": {
        "id": "-Qd5k_5UX4e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_oos_scores(predictor, features_list):\n",
        "    predictor.eval()\n",
        "    oos_scores = []\n",
        "\n",
        "    for feat in features_list:\n",
        "        if len(feat.z8_states) == 0:\n",
        "            oos_scores.append(0.0)\n",
        "            continue\n",
        "\n",
        "        scores = []\n",
        "        for i in range(len(feat.z8_states)):\n",
        "            z8 = feat.z8_states[i].to(DEVICE)\n",
        "            z12 = feat.z12_states[i].to(DEVICE)\n",
        "\n",
        "            top5_probs = [p for _, p in feat.top5_tokens[i]]\n",
        "            while len(top5_probs) < 5:\n",
        "                top5_probs.append(0.0)\n",
        "            logits_summary = torch.tensor([feat.token_entropies[i]] + top5_probs[:5], dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                score = predictor.compute_oos_score(z8, z12, logits_summary)\n",
        "            scores.append(score.item())\n",
        "\n",
        "        oos_scores.append(np.mean(scores))\n",
        "\n",
        "    return oos_scores"
      ],
      "metadata": {
        "id": "FCJezIuzX4c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_paraphrase_stats(sim_matrix, n_questions, group_size=5):\n",
        "    \"\"\"Compute within-group similarity stats.\"\"\"\n",
        "    stats = []\n",
        "    for idx in range(n_questions):\n",
        "        group_start = (idx // group_size) * group_size\n",
        "        group_end = min(group_start + group_size, n_questions)\n",
        "\n",
        "        same_group_sims = [sim_matrix[idx, j] for j in range(group_start, group_end) if j != idx]\n",
        "\n",
        "        if same_group_sims:\n",
        "            stats.append({\n",
        "                'mean_sim': np.mean(same_group_sims),\n",
        "                'std_sim': np.std(same_group_sims),\n",
        "                'min_sim': np.min(same_group_sims),\n",
        "            })\n",
        "        else:\n",
        "            stats.append({'mean_sim': 1.0, 'std_sim': 0.0, 'min_sim': 1.0})\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "zhix9SZFX4Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumes `model` and `tokenizer` are already loaded\n",
        "# Load data\n",
        "df = pd.read_csv(\"AI_Bullshit_Detector/data/paraphase-prompts.csv\")\n",
        "questions = df['question'].tolist()\n",
        "\n",
        "# Step 1: Extract features\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: Extracting internal features...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_features = []\n",
        "for question in tqdm(questions, desc=\"Processing\"):\n",
        "    try:\n",
        "        features = extract_hallucination_features(model, tokenizer, question, max_new_tokens=100)\n",
        "        all_features.append(features)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        all_features.append(HallucinationFeatures(prompt=\"\", question=question, input_length=0))\n",
        "\n",
        "df['llm_output'] = [f.full_text for f in all_features]\n",
        "df['mean_prob'] = [f.mean_prob for f in all_features]\n",
        "df['mean_entropy'] = [f.mean_entropy for f in all_features]\n",
        "df['head_disagree_8'] = [f.mean_head_disagreement_8 for f in all_features]\n",
        "df['head_disagree_12'] = [f.mean_head_disagreement_12 for f in all_features]\n",
        "\n",
        "# Step 2: Train predictor\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 2: Training z8 → z12 → logits predictor...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "predictor = train_predictor(all_features, epochs=20, lr=1e-4)\n",
        "\n",
        "# Step 3: OOS scores\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 3: Computing OOS scores...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df['oos_score'] = compute_oos_scores(predictor, all_features)\n",
        "\n",
        "# Step 4: Load similarity matrix\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 4: Loading paraphrase similarity...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sim_df = pd.read_csv(\"AI_Bullshit_Detector/data/llm_output_cosine_similarity.csv\", index_col=0)\n",
        "sim_matrix = sim_df.values\n",
        "paraphrase_stats = compute_paraphrase_stats(sim_matrix, len(questions), group_size=5)\n",
        "\n",
        "df['paraphrase_mean_sim'] = [s['mean_sim'] for s in paraphrase_stats]\n",
        "df['paraphrase_std_sim'] = [s['std_sim'] for s in paraphrase_stats]\n",
        "\n",
        "# Step 5: Combined score\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 5: Computing combined score...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "oos_norm = scaler.fit_transform(df[['oos_score']].fillna(0))\n",
        "head_norm = scaler.fit_transform(df[['head_disagree_12']].fillna(0))\n",
        "sim_inverted = 1 - df['paraphrase_mean_sim'].fillna(1).values.reshape(-1, 1)\n",
        "sim_norm = scaler.fit_transform(sim_inverted)\n",
        "\n",
        "df['combined_score'] = 0.4 * oos_norm.flatten() + 0.3 * head_norm.flatten() + 0.3 * sim_norm.flatten()\n",
        "\n",
        "print(f\"\\n✅ Done! Combined score range: [{df['combined_score'].min():.3f}, {df['combined_score'].max():.3f}]\")\n"
      ],
      "metadata": {
        "id": "kfJ7ZXnPX4X_",
        "outputId": "a808d8e0-4014-4ec7-9af7-989ad38e499b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 1: Extracting internal features...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing:  18%|█▊        | 18/100 [00:52<04:02,  2.96s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(16, 11))\n",
        "\n",
        "# 1. OOS vs Paraphrase\n",
        "ax = axes[0, 0]\n",
        "scatter = ax.scatter(df['oos_score'], df['paraphrase_mean_sim'], alpha=0.6, c=df['mean_entropy'], cmap='viridis', s=50)\n",
        "plt.colorbar(scatter, ax=ax, label='Mean Entropy')\n",
        "ax.set_xlabel('OOS Score (z8→z12 error)')\n",
        "ax.set_ylabel('Paraphrase Mean Similarity')\n",
        "ax.set_title('OOS Score vs Paraphrase Consistency')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Head Disagree vs Paraphrase Spread\n",
        "ax = axes[0, 1]\n",
        "ax.scatter(df['head_disagree_12'], df['paraphrase_std_sim'], alpha=0.6, s=50)\n",
        "ax.set_xlabel('Head Disagreement (Layer 12)')\n",
        "ax.set_ylabel('Paraphrase Sim Spread (std)')\n",
        "ax.set_title('Head Disagreement vs Instability')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Correlation heatmap\n",
        "ax = axes[0, 2]\n",
        "score_cols = ['oos_score', 'head_disagree_8', 'head_disagree_12', 'mean_entropy', 'mean_prob', 'paraphrase_mean_sim', 'paraphrase_std_sim']\n",
        "corr = df[score_cols].corr()\n",
        "sns.heatmap(corr, annot=True, fmt='.2f', ax=ax, cmap='RdBu_r', center=0, square=True)\n",
        "ax.set_title('Score Correlations')\n",
        "\n",
        "# 4. Layer 8 vs 12\n",
        "ax = axes[1, 0]\n",
        "ax.scatter(df['head_disagree_8'], df['head_disagree_12'], alpha=0.6, s=50)\n",
        "ax.plot([df['head_disagree_8'].min(), df['head_disagree_8'].max()],\n",
        "        [df['head_disagree_8'].min(), df['head_disagree_8'].max()], 'r--', label='y=x')\n",
        "ax.set_xlabel('Head Disagree (L8)')\n",
        "ax.set_ylabel('Head Disagree (L12)')\n",
        "ax.set_title('Layer 8 vs 12 Disagreement')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Combined score distribution\n",
        "ax = axes[1, 1]\n",
        "ax.hist(df['combined_score'], bins=25, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax.axvline(df['combined_score'].quantile(0.9), color='red', linestyle='--', lw=2, label=f'90th: {df[\"combined_score\"].quantile(0.9):.3f}')\n",
        "ax.axvline(df['combined_score'].median(), color='orange', linestyle='-', lw=2, label=f'Median: {df[\"combined_score\"].median():.3f}')\n",
        "ax.set_xlabel('Combined Score')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title('Combined Score Distribution')\n",
        "ax.legend()\n",
        "\n",
        "# 6. Top hallucinations\n",
        "ax = axes[1, 2]\n",
        "top10 = df.nlargest(10, 'combined_score')[['question', 'combined_score']]\n",
        "ax.barh(range(10), top10['combined_score'], color='coral', edgecolor='black')\n",
        "ax.set_yticks(range(10))\n",
        "ax.set_yticklabels([q[:45] + '...' if len(q) > 45 else q for q in top10['question']], fontsize=8)\n",
        "ax.set_xlabel('Combined Score')\n",
        "ax.set_title('Top 10 Suspected Hallucinations')\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('hallucination_analysis.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D_o6AAiBX4Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SCORE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "metrics = {\n",
        "    'OOS Score': 'oos_score',\n",
        "    'Head Disagree (L8)': 'head_disagree_8',\n",
        "    'Head Disagree (L12)': 'head_disagree_12',\n",
        "    'Mean Entropy': 'mean_entropy',\n",
        "    'Mean Prob': 'mean_prob',\n",
        "    'Paraphrase Mean Sim': 'paraphrase_mean_sim',\n",
        "    'Paraphrase Std Sim': 'paraphrase_std_sim',\n",
        "    'Combined Score': 'combined_score',\n",
        "}\n",
        "\n",
        "print(f\"{'Metric':<22} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
        "print(\"-\" * 62)\n",
        "for name, col in metrics.items():\n",
        "    print(f\"{name:<22} {df[col].mean():>10.4f} {df[col].std():>10.4f} {df[col].min():>10.4f} {df[col].max():>10.4f}\")\n",
        "\n",
        "# Save results\n",
        "df.to_csv('hallucination_scores.csv', index=False)\n",
        "print(\"\\n✅ Results saved to hallucination_scores.csv\")"
      ],
      "metadata": {
        "id": "4ViNH8bgYwV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vjwhTIURYwQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nlJTygwUYwLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5h-4meHX4N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUIbFdcKLfUv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from lion_pytorch import Lion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create template and tokenizer for LLM model to be able to chat with it"
      ],
      "metadata": {
        "id": "CbaGATN9ejrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic question-answer template\n",
        "template_without_answer = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "template_with_answer = template_without_answer + \"{answer}<|im_end|>\\n\"\n",
        "\n",
        "# Let's try to put something into the template to see how it looks\n",
        "print(template_with_answer.format(question=\"What is your name?\", answer=\"My name is Lili!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfUiPGf4eXCy",
        "outputId": "3d6cd44b-7a3b-469f-a77d-6f4f6c53bcf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|startoftext|><|im_start|>user\n",
            "What is your name?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "My name is Lili!<|im_end|>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for Liquid AI LFM2-1.2B\n",
        "model_id = \"LiquidAI/LFM2-1.2B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# How big is the tokenizer?\n",
        "print(f\"Vocab size: {len(tokenizer.get_vocab())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GulkJu1ieeKx",
        "outputId": "d80f1a2e-cbad-4315-8711-6b85eb14dd3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 64400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets test out both steps:\n",
        "text = \"Here is some sample text!\"\n",
        "print(f\"Original text: {text}\")\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "print(f\"Encoded tokens: {tokens}\")\n",
        "\n",
        "# Decode the tokens\n",
        "decoded_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "print(f\"Decoded text: {decoded_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq0suq7Teip2",
        "outputId": "141c954f-2e91-4948-aaa2-136dd9d98be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: Here is some sample text!\n",
            "Encoded tokens: tensor([[   1, 9151,  856, 1429, 6643, 3304,  510]])\n",
            "Decoded text: Here is some sample text!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template_without_answer.format(question=\"What is the capital of France? Use one word.\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rouLHxijeyB_",
        "outputId": "bc9bac1a-ecc0-4e4a-996f-be362bd986ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|startoftext|><|im_start|>user\n",
            "What is the capital of France? Use one word.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model -- note that this may take a few minutes\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "Qulnj7PyeWpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for Liquid AI LFM2-1.2B\n",
        "model_id = \"LiquidAI/LFM2-1.2B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# How big is the tokenizer?\n",
        "print(f\"Vocab size: {len(tokenizer.get_vocab())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UYbJip4yh-3",
        "outputId": "2852300b-45dd-4570-aef4-46cae633545a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 64400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "rQSwjoG2ysXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template_without_answer.format(question=\"What does MIT stand for?\")\n",
        "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "output = model.generate(tokens, max_new_tokens=20)\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi4ylBriuCO9",
        "outputId": "c30836d6-8d95-4179-d411-0f39b3ad934e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|startoftext|><|startoftext|><|im_start|>user\n",
            "What does MIT stand for?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "MIT stands for Massachusetts Institute of Technology. It is a private, research-intensive university located in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the internal representations of the model"
      ],
      "metadata": {
        "id": "BAZyrnmNFaeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the model structure\n",
        "print(model.config)\n",
        "print(model)\n",
        "\n",
        "# Check if the model has a specific cache class\n",
        "from transformers import Cache\n",
        "print(f\"Cache class: {getattr(model.config, 'cache_implementation', 'default')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO8gk8ugFaEn",
        "outputId": "213ec8b8-dcb4-4c6a-f301-0398961ade84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lfm2Config {\n",
            "  \"architectures\": [\n",
            "    \"Lfm2ForCausalLM\"\n",
            "  ],\n",
            "  \"block_auto_adjust_ff_dim\": true,\n",
            "  \"block_dim\": 2048,\n",
            "  \"block_ff_dim\": 12288,\n",
            "  \"block_ffn_dim_multiplier\": 1.0,\n",
            "  \"block_mlp_init_scale\": 1.0,\n",
            "  \"block_multiple_of\": 256,\n",
            "  \"block_norm_eps\": 1e-05,\n",
            "  \"block_out_init_scale\": 1.0,\n",
            "  \"block_use_swiglu\": true,\n",
            "  \"block_use_xavier_init\": true,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"conv_L_cache\": 3,\n",
            "  \"conv_bias\": false,\n",
            "  \"conv_dim\": 2048,\n",
            "  \"conv_dim_out\": 2048,\n",
            "  \"conv_use_xavier_init\": true,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"eos_token_id\": 7,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"layer_types\": [\n",
            "    \"conv\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\",\n",
            "    \"full_attention\",\n",
            "    \"conv\"\n",
            "  ],\n",
            "  \"max_position_embeddings\": 128000,\n",
            "  \"model_type\": \"lfm2\",\n",
            "  \"norm_eps\": 1e-05,\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_heads\": 32,\n",
            "  \"num_hidden_layers\": 16,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"transformers_version\": \"4.57.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_pos_enc\": true,\n",
            "  \"vocab_size\": 65536\n",
            "}\n",
            "\n",
            "Lfm2ForCausalLM(\n",
            "  (model): Lfm2Model(\n",
            "    (embed_tokens): Embedding(65536, 2048, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (2): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (3-4): 2 x Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (5): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (6-7): 2 x Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (8): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (9): Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (10): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (11): Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (12): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (13): Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (14): Lfm2DecoderLayer(\n",
            "        (self_attn): Lfm2Attention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (q_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "          (k_layernorm): Lfm2RMSNorm((64,), eps=1e-05)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "      (15): Lfm2DecoderLayer(\n",
            "        (conv): Lfm2ShortConv(\n",
            "          (conv): Conv1d(2048, 2048, kernel_size=(3,), stride=(1,), padding=(2,), groups=2048, bias=False)\n",
            "          (in_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
            "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (feed_forward): Lfm2MLP(\n",
            "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "        )\n",
            "        (operator_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "        (ffn_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (rotary_emb): Lfm2RotaryEmbedding()\n",
            "    (pos_emb): Lfm2RotaryEmbedding()\n",
            "    (embedding_norm): Lfm2RMSNorm((2048,), eps=1e-05)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=65536, bias=False)\n",
            ")\n",
            "Cache class: default\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode prompt\n",
        "prompt = template_without_answer.format(question=\"What does MIT stand for?\")\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Forward pass with cache\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids,\n",
        "        use_cache=True,\n",
        "        return_dict=True,\n",
        "        output_attentions=True,  # Optional: attention weights\n",
        "        output_hidden_states=True  # Optional: hidden states\n",
        "    )\n",
        "\n",
        "# Access cache\n",
        "past_key_values = outputs.past_key_values\n",
        "logits = outputs.logits"
      ],
      "metadata": {
        "id": "Qhru7ph8Fpdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect cache structure\n",
        "print(f\"Cache type: {type(past_key_values)}\")\n",
        "print(f\"Number of layers in cache: {len(past_key_values)}\")\n",
        "\n",
        "# For each layer, check the structure\n",
        "for i, layer_cache in enumerate(past_key_values):\n",
        "    if layer_cache is not None:\n",
        "        if isinstance(layer_cache, tuple):\n",
        "            print(f\"Layer {i}: K shape={layer_cache[0].shape}, V shape={layer_cache[1].shape}\")\n",
        "        else:\n",
        "            print(f\"Layer {i}: type={type(layer_cache)}\")\n",
        "    else:\n",
        "        print(f\"Layer {i}: None (likely conv layer)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK73jURaFx1O",
        "outputId": "411e62d5-5590-416d-e8dd-7445b17df2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache type: <class 'transformers.models.lfm2.modeling_lfm2.Lfm2HybridConvCache'>\n",
            "Number of layers in cache: 15\n",
            "Layer 0: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 1: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 2: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n",
            "Layer 3: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 4: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 5: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n",
            "Layer 6: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 7: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 8: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n",
            "Layer 9: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 10: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n",
            "Layer 11: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 12: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n",
            "Layer 13: K shape=torch.Size([0]), V shape=torch.Size([0])\n",
            "Layer 14: K shape=torch.Size([1, 8, 16, 64]), V shape=torch.Size([1, 8, 16, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_cache_and_probs(model, tokenizer, prompt, max_new_tokens=20):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    generated_tokens = []\n",
        "    token_probs = []\n",
        "    all_caches = []\n",
        "\n",
        "    past_key_values = None\n",
        "    current_ids = input_ids\n",
        "\n",
        "    for step in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                current_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "        # Get logits for last token\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        # Compute probabilities\n",
        "        probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "        # Sample or greedy decode\n",
        "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # Get probability of selected token\n",
        "        selected_prob = probs[0, next_token.item()].item()\n",
        "\n",
        "        # Store results\n",
        "        generated_tokens.append(next_token.item())\n",
        "        token_probs.append(selected_prob)\n",
        "        all_caches.append(outputs.past_key_values)\n",
        "\n",
        "        # Check for EOS\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        # Update for next iteration\n",
        "        past_key_values = outputs.past_key_values\n",
        "        current_ids = next_token\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"generated_tokens\": generated_tokens,\n",
        "        \"token_probs\": token_probs,\n",
        "        \"final_cache\": all_caches[-1],\n",
        "        \"text\": tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    }"
      ],
      "metadata": {
        "id": "Cxq5dcVvGDVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run generation\n",
        "result = generate_with_cache_and_probs(\n",
        "    model, tokenizer,\n",
        "    template_without_answer.format(question=\"What does MIT stand for?\"),\n",
        "    max_new_tokens=20\n",
        ")\n",
        "\n",
        "# Display output\n",
        "print(f\"Generated text: {result['text']}\")\n",
        "print(f\"\\nToken-by-token breakdown:\")\n",
        "for i, (tok, prob) in enumerate(zip(result['generated_tokens'], result['token_probs'])):\n",
        "    print(f\"  Step {i}: '{tokenizer.decode([tok])}' (prob: {prob:.4f})\")\n",
        "\n",
        "# Analyze final KV cache\n",
        "final_cache = result['final_cache']\n",
        "print(f\"\\nFinal KV Cache Summary:\")\n",
        "for i, layer_cache in enumerate(final_cache):\n",
        "    if layer_cache is not None and isinstance(layer_cache, tuple):\n",
        "        k, v = layer_cache\n",
        "        print(f\"  Layer {i}: K={k.shape}, V={v.shape}, K_mem={k.element_size()*k.nelement()/1024:.1f}KB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls0jorOHGHZ5",
        "outputId": "fb7e4296-b6a8-46b0-ca8d-11f30dc67d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: MIT stands for Massachusetts Institute of Technology. It is a private, research-intensive university located in\n",
            "\n",
            "Token-by-token breakdown:\n",
            "  Step 0: 'M' (prob: 0.9520)\n",
            "  Step 1: 'IT' (prob: 0.9993)\n",
            "  Step 2: ' stands' (prob: 0.9243)\n",
            "  Step 3: ' for' (prob: 0.9999)\n",
            "  Step 4: ' Massachusetts' (prob: 0.8794)\n",
            "  Step 5: ' Institute' (prob: 0.9990)\n",
            "  Step 6: ' of' (prob: 0.9999)\n",
            "  Step 7: ' Technology' (prob: 0.9993)\n",
            "  Step 8: '.' (prob: 0.8819)\n",
            "  Step 9: ' It' (prob: 0.8163)\n",
            "  Step 10: ' is' (prob: 0.5875)\n",
            "  Step 11: ' a' (prob: 0.8055)\n",
            "  Step 12: ' private' (prob: 0.3536)\n",
            "  Step 13: ',' (prob: 0.6832)\n",
            "  Step 14: ' research' (prob: 0.7299)\n",
            "  Step 15: '-int' (prob: 0.6954)\n",
            "  Step 16: 'ensive' (prob: 0.9982)\n",
            "  Step 17: ' university' (prob: 0.1610)\n",
            "  Step 18: ' located' (prob: 0.9285)\n",
            "  Step 19: ' in' (prob: 0.9984)\n",
            "\n",
            "Final KV Cache Summary:\n",
            "  Layer 0: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 1: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 2: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n",
            "  Layer 3: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 4: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 5: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n",
            "  Layer 6: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 7: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 8: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n",
            "  Layer 9: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 10: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n",
            "  Layer 11: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 12: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n",
            "  Layer 13: K=torch.Size([0]), V=torch.Size([0]), K_mem=0.0KB\n",
            "  Layer 14: K=torch.Size([1, 8, 35, 64]), V=torch.Size([1, 8, 35, 64]), K_mem=70.0KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_kv_cache(cache):\n",
        "    \"\"\"Analyze KV cache for hybrid LFM2 model\"\"\"\n",
        "    stats = {\n",
        "        \"total_layers\": len(cache),\n",
        "        \"attention_layers\": 0,\n",
        "        \"total_memory_bytes\": 0,\n",
        "        \"layer_details\": []\n",
        "    }\n",
        "\n",
        "    for i, layer_cache in enumerate(cache):\n",
        "        if layer_cache is not None and isinstance(layer_cache, tuple):\n",
        "            k, v = layer_cache\n",
        "            layer_mem = (k.element_size() * k.nelement() +\n",
        "                        v.element_size() * v.nelement())\n",
        "            stats[\"attention_layers\"] += 1\n",
        "            stats[\"total_memory_bytes\"] += layer_mem\n",
        "            stats[\"layer_details\"].append({\n",
        "                \"layer\": i,\n",
        "                \"k_shape\": list(k.shape),\n",
        "                \"v_shape\": list(v.shape),\n",
        "                \"memory_kb\": layer_mem / 1024\n",
        "            })\n",
        "\n",
        "    return stats\n",
        "\n",
        "cache_stats = analyze_kv_cache(result['final_cache'])\n",
        "print(json.dumps(cache_stats, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEwQ_ohIGORj",
        "outputId": "02a20ba3-04a5-491c-e7bd-cc67bbda55ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"total_layers\": 15,\n",
            "  \"attention_layers\": 15,\n",
            "  \"total_memory_bytes\": 860160,\n",
            "  \"layer_details\": [\n",
            "    {\n",
            "      \"layer\": 0,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 1,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 2,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 3,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 4,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 5,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 6,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 7,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 8,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 9,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 10,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 11,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 12,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 13,\n",
            "      \"k_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        0\n",
            "      ],\n",
            "      \"memory_kb\": 0.0\n",
            "    },\n",
            "    {\n",
            "      \"layer\": 14,\n",
            "      \"k_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"v_shape\": [\n",
            "        1,\n",
            "        8,\n",
            "        35,\n",
            "        64\n",
            "      ],\n",
            "      \"memory_kb\": 140.0\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Calculate the I Do Not Know Score"
      ],
      "metadata": {
        "id": "eJS6QLFqUWm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1"
      ],
      "metadata": {
        "id": "BzbnjcPSU2WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class HallucinationFeatures:\n",
        "    \"\"\"Features extracted for hallucination detection.\"\"\"\n",
        "    # Input\n",
        "    prompt: str\n",
        "    input_ids: torch.Tensor\n",
        "\n",
        "    # Hidden states (per generated token)\n",
        "    z8_states: List[torch.Tensor]   # Layer 8 hidden states\n",
        "    z12_states: List[torch.Tensor]  # Layer 12 hidden states\n",
        "\n",
        "    # KV cache stats (per generated token)\n",
        "    kv8_head_disagreement: List[float]\n",
        "    kv12_head_disagreement: List[float]\n",
        "\n",
        "    # Output\n",
        "    generated_tokens: List[int]\n",
        "    token_probs: List[float]\n",
        "    token_entropies: List[float]\n",
        "    top5_tokens: List[List[tuple]]\n",
        "    full_text: str\n",
        "\n",
        "    # Summary scores\n",
        "    mean_prob: float\n",
        "    mean_entropy: float\n",
        "    mean_head_disagreement_8: float\n",
        "    mean_head_disagreement_12: float\n",
        "\n",
        "\n",
        "def compute_head_disagreement(kv_cache_layer) -> float:\n",
        "    \"\"\"\n",
        "    Compute disagreement across KV heads for a single layer.\n",
        "    Higher = more disagreement = potential hallucination signal.\n",
        "\n",
        "    kv_cache_layer: tuple of (K, V) each with shape [batch, 8_heads, seq, 64]\n",
        "    \"\"\"\n",
        "    k, v = kv_cache_layer\n",
        "\n",
        "    if k.numel() == 0:  # Conv layer, no KV cache\n",
        "        return 0.0\n",
        "\n",
        "    # Variance across heads (dim=1) for the LAST token position\n",
        "    # Shape: [batch, heads, head_dim] -> variance over heads\n",
        "    k_last = k[:, :, -1, :]  # [1, 8, 64]\n",
        "    v_last = v[:, :, -1, :]\n",
        "\n",
        "    k_var = k_last.var(dim=1).mean().item()  # Variance across 8 heads\n",
        "    v_var = v_last.var(dim=1).mean().item()\n",
        "\n",
        "    return k_var + v_var\n",
        "\n",
        "\n",
        "def extract_hallucination_features(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 50\n",
        ") -> HallucinationFeatures:\n",
        "    \"\"\"\n",
        "    Generate tokens while extracting all features needed for hallucination detection.\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Storage\n",
        "    z8_states = []\n",
        "    z12_states = []\n",
        "    kv8_disagreement = []\n",
        "    kv12_disagreement = []\n",
        "    generated_tokens = []\n",
        "    token_probs = []\n",
        "    token_entropies = []\n",
        "    top5_tokens = []\n",
        "\n",
        "    past_key_values = None\n",
        "    current_ids = input_ids\n",
        "\n",
        "    for step in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                current_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "                output_hidden_states=True,  # KEY: Get layer outputs\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "        # === HIDDEN STATES (z8, z12) ===\n",
        "        # hidden_states is tuple of (embedding, layer0, layer1, ..., layer15)\n",
        "        # So layer 8 output is at index 9, layer 12 is at index 13\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "        # Get last token's hidden state for the current step\n",
        "        z8 = hidden_states[9][:, -1, :].clone().cpu()   # [1, 2048]\n",
        "        z12 = hidden_states[13][:, -1, :].clone().cpu() # [1, 2048]\n",
        "\n",
        "        z8_states.append(z8)\n",
        "        z12_states.append(z12)\n",
        "\n",
        "        # === KV CACHE HEAD DISAGREEMENT ===\n",
        "        cache = outputs.past_key_values\n",
        "        kv8_disagreement.append(compute_head_disagreement(cache[8]))\n",
        "        kv12_disagreement.append(compute_head_disagreement(cache[12]))\n",
        "\n",
        "        # === LOGITS & PROBABILITIES ===\n",
        "        logits = outputs.logits[:, -1, :]  # [1, vocab_size]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Entropy of distribution\n",
        "        entropy = -(probs * torch.log(probs + 1e-10)).sum().item()\n",
        "\n",
        "        # Greedy selection\n",
        "        next_token = torch.argmax(logits, dim=-1)\n",
        "        selected_prob = probs[0, next_token.item()].item()\n",
        "\n",
        "        # Top-5\n",
        "        top5_probs, top5_idx = torch.topk(probs[0], 5)\n",
        "        top5 = [(tokenizer.decode([idx.item()]), prob.item())\n",
        "                for idx, prob in zip(top5_idx, top5_probs)]\n",
        "\n",
        "        # Store\n",
        "        generated_tokens.append(next_token.item())\n",
        "        token_probs.append(selected_prob)\n",
        "        token_entropies.append(entropy)\n",
        "        top5_tokens.append(top5)\n",
        "\n",
        "        # Check EOS\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        # Update for next step\n",
        "        past_key_values = outputs.past_key_values\n",
        "        current_ids = next_token.unsqueeze(0)\n",
        "\n",
        "    # === BUILD RESULT ===\n",
        "    return HallucinationFeatures(\n",
        "        prompt=prompt,\n",
        "        input_ids=input_ids.cpu(),\n",
        "        z8_states=z8_states,\n",
        "        z12_states=z12_states,\n",
        "        kv8_head_disagreement=kv8_disagreement,\n",
        "        kv12_head_disagreement=kv12_disagreement,\n",
        "        generated_tokens=generated_tokens,\n",
        "        token_probs=token_probs,\n",
        "        token_entropies=token_entropies,\n",
        "        top5_tokens=top5_tokens,\n",
        "        full_text=tokenizer.decode(generated_tokens, skip_special_tokens=True),\n",
        "        mean_prob=np.mean(token_probs),\n",
        "        mean_entropy=np.mean(token_entropies),\n",
        "        mean_head_disagreement_8=np.mean(kv8_disagreement),\n",
        "        mean_head_disagreement_12=np.mean(kv12_disagreement),\n",
        "    )"
      ],
      "metadata": {
        "id": "KVQXnQWbUdNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Unsupervised z8 → z12 Predictor"
      ],
      "metadata": {
        "id": "ItPD3gesU4ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Z8toZ12Predictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Predict z12 from z8. High prediction error = out-of-support.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=2048, bottleneck_dim=512):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(bottleneck_dim),\n",
        "            nn.Linear(bottleneck_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(bottleneck_dim),\n",
        "            nn.Linear(bottleneck_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, z8):\n",
        "        return self.net(z8)\n",
        "\n",
        "    def prediction_error(self, z8, z12):\n",
        "        \"\"\"MSE between predicted and actual z12.\"\"\"\n",
        "        z12_pred = self.forward(z8)\n",
        "        return F.mse_loss(z12_pred, z12, reduction='none').mean(dim=-1)\n",
        "\n",
        "\n",
        "class Z8Z12LogitsPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extended predictor: z8 → z12 → logits_summary\n",
        "    Captures full information flow consistency.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim=2048, vocab_size=65536, bottleneck_dim=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # z8 → z12 predictor\n",
        "        self.z8_to_z12 = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(bottleneck_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "        # z12 → logits summary (top-k probs, entropy)\n",
        "        # We predict a summary, not full vocab logits\n",
        "        self.z12_to_logits_summary = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, bottleneck_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(bottleneck_dim, 6),  # [entropy, top1_prob, top2_prob, ...]\n",
        "        )\n",
        "\n",
        "    def forward(self, z8):\n",
        "        z12_pred = self.z8_to_z12(z8)\n",
        "        logits_summary_pred = self.z12_to_logits_summary(z12_pred)\n",
        "        return z12_pred, logits_summary_pred\n",
        "\n",
        "    def compute_oos_score(self, z8, z12, logits_summary):\n",
        "        \"\"\"\n",
        "        Out-of-support score combining both prediction errors.\n",
        "        \"\"\"\n",
        "        z12_pred, logits_pred = self.forward(z8)\n",
        "\n",
        "        err_z12 = F.mse_loss(z12_pred, z12, reduction='none').mean(dim=-1)\n",
        "        err_logits = F.mse_loss(logits_pred, logits_summary, reduction='none').mean(dim=-1)\n",
        "\n",
        "        # Weighted combination\n",
        "        return 0.7 * err_z12 + 0.3 * err_logits\n",
        "\n",
        "\n",
        "class HallucinationDataset(Dataset):\n",
        "    \"\"\"Dataset for training the predictor.\"\"\"\n",
        "    def __init__(self, features_list: List[HallucinationFeatures]):\n",
        "        self.samples = []\n",
        "\n",
        "        for feat in features_list:\n",
        "            for i, (z8, z12, prob, entropy) in enumerate(zip(\n",
        "                feat.z8_states,\n",
        "                feat.z12_states,\n",
        "                feat.token_probs,\n",
        "                feat.token_entropies\n",
        "            )):\n",
        "                # Create logits summary: [entropy, top5_probs]\n",
        "                top5_probs = [p for _, p in feat.top5_tokens[i]]\n",
        "                logits_summary = torch.tensor([entropy] + top5_probs, dtype=torch.float32)\n",
        "\n",
        "                self.samples.append({\n",
        "                    'z8': z8.squeeze(0),\n",
        "                    'z12': z12.squeeze(0),\n",
        "                    'logits_summary': logits_summary,\n",
        "                    'prob': prob,\n",
        "                    'entropy': entropy,\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "def train_predictor(features_list: List[HallucinationFeatures], epochs=10, lr=1e-4):\n",
        "    \"\"\"Train the z8 → z12 → logits predictor.\"\"\"\n",
        "    dataset = HallucinationDataset(features_list)\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    predictor = Z8Z12LogitsPredictor()\n",
        "    optimizer = torch.optim.AdamW(predictor.parameters(), lr=lr)\n",
        "\n",
        "    predictor.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in loader:\n",
        "            z8 = batch['z8']\n",
        "            z12 = batch['z12']\n",
        "            logits_summary = batch['logits_summary']\n",
        "\n",
        "            z12_pred, logits_pred = predictor(z8)\n",
        "\n",
        "            loss_z12 = F.mse_loss(z12_pred, z12)\n",
        "            loss_logits = F.mse_loss(logits_pred, logits_summary)\n",
        "            loss = 0.7 * loss_z12 + 0.3 * loss_logits\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(loader):.6f}\")\n",
        "\n",
        "    return predictor"
      ],
      "metadata": {
        "id": "R8oh2sWOUzQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Head Disagreement Score (Refined)"
      ],
      "metadata": {
        "id": "xiskv8KcVAzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sequence_head_disagreement(features: HallucinationFeatures) -> Dict:\n",
        "    \"\"\"\n",
        "    Compute detailed head disagreement statistics.\n",
        "    \"\"\"\n",
        "    kv8 = np.array(features.kv8_head_disagreement)\n",
        "    kv12 = np.array(features.kv12_head_disagreement)\n",
        "\n",
        "    return {\n",
        "        # Layer 8\n",
        "        \"kv8_mean\": kv8.mean(),\n",
        "        \"kv8_std\": kv8.std(),\n",
        "        \"kv8_max\": kv8.max(),\n",
        "        \"kv8_trend\": np.polyfit(range(len(kv8)), kv8, 1)[0] if len(kv8) > 1 else 0,\n",
        "\n",
        "        # Layer 12\n",
        "        \"kv12_mean\": kv12.mean(),\n",
        "        \"kv12_std\": kv12.std(),\n",
        "        \"kv12_max\": kv12.max(),\n",
        "        \"kv12_trend\": np.polyfit(range(len(kv12)), kv12, 1)[0] if len(kv12) > 1 else 0,\n",
        "\n",
        "        # Cross-layer\n",
        "        \"kv_ratio\": kv12.mean() / (kv8.mean() + 1e-8),\n",
        "        \"kv_correlation\": np.corrcoef(kv8, kv12)[0, 1] if len(kv8) > 1 else 0,\n",
        "    }"
      ],
      "metadata": {
        "id": "qq5_nzH1VC5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Integration with Paraphrase Dataset"
      ],
      "metadata": {
        "id": "EIavMpA8VFUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_full_hallucination_analysis(model, tokenizer, csv_path: str):\n",
        "    \"\"\"\n",
        "    Run complete analysis pipeline on paraphrase dataset.\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # === STEP 1: Extract features for all questions ===\n",
        "    print(\"Extracting internal features...\")\n",
        "    all_features = []\n",
        "    for question in tqdm(df['question'], desc=\"Processing\"):\n",
        "        prompt = template_without_answer.format(question=question)\n",
        "        features = extract_hallucination_features(model, tokenizer, prompt, max_new_tokens=100)\n",
        "        all_features.append(features)\n",
        "\n",
        "    df['llm_output'] = [f.full_text for f in all_features]\n",
        "    df['mean_prob'] = [f.mean_prob for f in all_features]\n",
        "    df['mean_entropy'] = [f.mean_entropy for f in all_features]\n",
        "    df['head_disagree_8'] = [f.mean_head_disagreement_8 for f in all_features]\n",
        "    df['head_disagree_12'] = [f.mean_head_disagreement_12 for f in all_features]\n",
        "\n",
        "    # === STEP 2: Train z8 → z12 predictor ===\n",
        "    print(\"\\nTraining z8 → z12 predictor...\")\n",
        "    predictor = train_predictor(all_features, epochs=20)\n",
        "\n",
        "    # === STEP 3: Compute OOS scores ===\n",
        "    print(\"\\nComputing out-of-support scores...\")\n",
        "    predictor.eval()\n",
        "    oos_scores = []\n",
        "\n",
        "    for feat in all_features:\n",
        "        scores = []\n",
        "        for z8, z12, prob, entropy in zip(\n",
        "            feat.z8_states, feat.z12_states,\n",
        "            feat.token_probs, feat.token_entropies\n",
        "        ):\n",
        "            top5_probs = [p for _, p in feat.top5_tokens[feat.z8_states.index(z8)]]\n",
        "            logits_summary = torch.tensor([entropy] + top5_probs[:5])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                score = predictor.compute_oos_score(\n",
        "                    z8.squeeze(0).unsqueeze(0),\n",
        "                    z12.squeeze(0).unsqueeze(0),\n",
        "                    logits_summary.unsqueeze(0)\n",
        "                )\n",
        "            scores.append(score.item())\n",
        "\n",
        "        oos_scores.append(np.mean(scores))\n",
        "\n",
        "    df['oos_score'] = oos_scores\n",
        "\n",
        "    # === STEP 4: Paraphrase consistency (TF-IDF) ===\n",
        "    print(\"\\nComputing paraphrase consistency...\")\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    embeddings = vectorizer.fit_transform(df['llm_output']).toarray()\n",
        "    embeddings = normalize(embeddings)\n",
        "    cos_sim_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "    # Group by category and compute consistency\n",
        "    paraphrase_stats = []\n",
        "    for idx, row in df.iterrows():\n",
        "        category = row['category']\n",
        "        same_category = df[df['category'] == category].index.tolist()\n",
        "\n",
        "        if len(same_category) > 1:\n",
        "            sims = [cos_sim_matrix[idx, j] for j in same_category if j != idx]\n",
        "            paraphrase_stats.append({\n",
        "                'mean_sim': np.mean(sims),\n",
        "                'std_sim': np.std(sims),\n",
        "                'min_sim': np.min(sims),\n",
        "            })\n",
        "        else:\n",
        "            paraphrase_stats.append({'mean_sim': 1.0, 'std_sim': 0.0, 'min_sim': 1.0})\n",
        "\n",
        "    df['paraphrase_mean_sim'] = [s['mean_sim'] for s in paraphrase_stats]\n",
        "    df['paraphrase_std_sim'] = [s['std_sim'] for s in paraphrase_stats]\n",
        "    df['paraphrase_min_sim'] = [s['min_sim'] for s in paraphrase_stats]\n",
        "\n",
        "    return df, predictor, all_features\n",
        "\n",
        "\n",
        "# === RUN ANALYSIS ===\n",
        "df_results, predictor, all_features = run_full_hallucination_analysis(\n",
        "    model, tokenizer,\n",
        "    \"AI_Bullshit_Detector/data/paraphase-prompts.csv\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "YZl-ZBdbVGv1",
        "outputId": "46062df3-a9fc-4a93-b346-abaf5dd8c6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting internal features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 100/100 [05:02<00:00,  3.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training z8 → z12 predictor...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 must have the same dtype, but got Half and Float",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1120777079.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# === RUN ANALYSIS ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m df_results, predictor, all_features = run_full_hallucination_analysis(\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m\"AI_Bullshit_Detector/data/paraphase-prompts.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1120777079.py\u001b[0m in \u001b[0;36mrun_full_hallucination_analysis\u001b[0;34m(model, tokenizer, csv_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# === STEP 2: Train z8 → z12 predictor ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining z8 → z12 predictor...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# === STEP 3: Compute OOS scores ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2454074899.py\u001b[0m in \u001b[0;36mtrain_predictor\u001b[0;34m(features_list, epochs, lr)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mlogits_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits_summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mz12_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mloss_z12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz12_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2454074899.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z8)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mz12_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz8_to_z12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mlogits_summary_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz12_to_logits_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz12_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz12_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_summary_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Half and Float"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Compare & Visualize Scores\n"
      ],
      "metadata": {
        "id": "klOhf1AJVOdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_score_comparison(df):\n",
        "    \"\"\"Compare all hallucination detection strategies.\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "    # 1. OOS Score vs Paraphrase Consistency\n",
        "    ax = axes[0, 0]\n",
        "    ax.scatter(df['oos_score'], df['paraphrase_mean_sim'], alpha=0.6, c=df['mean_entropy'], cmap='viridis')\n",
        "    ax.set_xlabel('OOS Score (z8→z12 error)')\n",
        "    ax.set_ylabel('Paraphrase Mean Similarity')\n",
        "    ax.set_title('OOS Score vs Paraphrase Consistency')\n",
        "\n",
        "    # 2. Head Disagreement vs Paraphrase Spread\n",
        "    ax = axes[0, 1]\n",
        "    ax.scatter(df['head_disagree_12'], df['paraphrase_std_sim'], alpha=0.6)\n",
        "    ax.set_xlabel('Head Disagreement (Layer 12)')\n",
        "    ax.set_ylabel('Paraphrase Similarity Spread')\n",
        "    ax.set_title('Head Disagreement vs Paraphrase Instability')\n",
        "\n",
        "    # 3. Correlation heatmap\n",
        "    ax = axes[0, 2]\n",
        "    score_cols = ['oos_score', 'head_disagree_8', 'head_disagree_12',\n",
        "                  'mean_entropy', 'mean_prob', 'paraphrase_mean_sim', 'paraphrase_std_sim']\n",
        "    corr = df[score_cols].corr()\n",
        "    sns.heatmap(corr, annot=True, fmt='.2f', ax=ax, cmap='RdBu_r', center=0)\n",
        "    ax.set_title('Score Correlations')\n",
        "\n",
        "    # 4. Distribution of OOS scores by category\n",
        "    ax = axes[1, 0]\n",
        "    df.boxplot(column='oos_score', by='category', ax=ax, rot=45)\n",
        "    ax.set_title('OOS Score Distribution by Category')\n",
        "\n",
        "    # 5. Combined hallucination score\n",
        "    # Normalize and combine\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    df['combined_score'] = (\n",
        "        0.4 * scaler.fit_transform(df[['oos_score']]) +\n",
        "        0.3 * scaler.fit_transform(df[['head_disagree_12']]) +\n",
        "        0.3 * (1 - scaler.fit_transform(df[['paraphrase_mean_sim']]))\n",
        "    ).flatten()\n",
        "\n",
        "    ax = axes[1, 1]\n",
        "    ax.hist(df['combined_score'], bins=30, edgecolor='black', alpha=0.7)\n",
        "    ax.set_xlabel('Combined Hallucination Score')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_title('Distribution of Combined Hallucination Score')\n",
        "    ax.axvline(df['combined_score'].quantile(0.9), color='red', linestyle='--', label='90th percentile')\n",
        "    ax.legend()\n",
        "\n",
        "    # 6. Top suspected hallucinations\n",
        "    ax = axes[1, 2]\n",
        "    top_hallucinations = df.nlargest(10, 'combined_score')[['question', 'combined_score']]\n",
        "    ax.barh(range(10), top_hallucinations['combined_score'])\n",
        "    ax.set_yticks(range(10))\n",
        "    ax.set_yticklabels([q[:40] + '...' for q in top_hallucinations['question']], fontsize=8)\n",
        "    ax.set_xlabel('Combined Score')\n",
        "    ax.set_title('Top 10 Suspected Hallucinations')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('hallucination_analysis.png', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "df_final = visualize_score_comparison(df_results)\n",
        "df_final.to_csv('hallucination_scores.csv', index=False)"
      ],
      "metadata": {
        "id": "FRfncA0zVPwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine similary of paraphrased sentences --> used in testing the Iinternal representation model (IRM) and output."
      ],
      "metadata": {
        "id": "q45u8Z1uOW31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# --- 1. Load CSV ---\n",
        "#!rm -rf AI_Bullshit_Detector\n",
        "# Clone the repo\n",
        "!git clone https://github.com/AvdMei/AI_Bullshit_Detector\n",
        "df = pd.read_csv(\"AI_Bullshit_Detector/data/paraphase-prompts.csv\")\n",
        "print(df.head())\n",
        "assert \"category\" in df.columns and \"question\" in df.columns\n",
        "# --- 2. Load Liquid AI model and tokenizer ---\n",
        "model_id = \"LiquidAI/LFM2-1.2B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "model.eval()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "# --- 3. Templates ---\n",
        "template_without_answer = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "# --- 4. Function to generate response ---\n",
        "def generate_llm_output(question, max_tokens=200):\n",
        "    prompt = template_without_answer.format(question=question)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens,\n",
        "            do_sample=False  # deterministic for cosine similarity\n",
        "        )\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    # Remove the prompt part to get only the assistant answer\n",
        "    if \"<|im_start|>assistant\" in output_text:\n",
        "        answer = output_text.split(\"<|im_start|>assistant\")[1].strip()\n",
        "    else:\n",
        "        answer = output_text\n",
        "    return answer\n",
        "# --- 5. Generate LLM outputs ---\n",
        "llm_outputs = []\n",
        "for q in tqdm(df['question'], desc=\"Generating outputs\"):\n",
        "    try:\n",
        "        out = generate_llm_output(q)\n",
        "    except Exception as e:\n",
        "        out = f\"ERROR: {e}\"\n",
        "    llm_outputs.append(out)\n",
        "df['llm_output'] = llm_outputs\n",
        "# --- 6. Convert outputs to embeddings using TF-IDF ---\n",
        "vectorizer = TfidfVectorizer()\n",
        "embeddings = vectorizer.fit_transform(df['llm_output']).toarray()\n",
        "embeddings = normalize(embeddings)\n",
        "# --- 7. Compute cosine similarity ---\n",
        "cos_sim_matrix = cosine_similarity(embeddings)\n",
        "# --- 8. Save similarity matrix to CSV ---\n",
        "sim_df = pd.DataFrame(cos_sim_matrix, index=df['question'], columns=df['question'])\n",
        "sim_df.to_csv(\"AI_Bullshit_Detector/data/llm_output_cosine_similarity.csv\")\n",
        "print(\"Cosine similarity matrix saved to data/llm_output_cosine_similarity.csv\")\n",
        "# --- 9. Save outputs alongside prompts ---\n",
        "df.to_csv(\"AI_Bullshit_Detector/data/paraphases_with_llm_output.csv\", index=False)\n",
        "print(\"LLM outputs saved to data/paraphases_with_llm_output.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHJk5UVnNaB3",
        "outputId": "8e2e2a29-dc83-4e73-89bd-bedef8c6e724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI_Bullshit_Detector'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (139/139), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 139 (delta 46), reused 92 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (139/139), 457.96 KiB | 7.27 MiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n",
            "                           category  \\\n",
            "0  Software Engineering - Debugging   \n",
            "1  Software Engineering - Debugging   \n",
            "2  Software Engineering - Debugging   \n",
            "3  Software Engineering - Debugging   \n",
            "4  Software Engineering - Debugging   \n",
            "\n",
            "                                            question  \n",
            "0  What is the first step an experienced engineer...  \n",
            "1  When a bug is reported, what should be done fi...  \n",
            "2  What is the initial action a senior engineer t...  \n",
            "3  Before changing code to fix a bug, what should...  \n",
            "4  What is the most important first step in a pro...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Generating outputs: 100%|██████████| 100/100 [07:49<00:00,  4.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity matrix saved to data/llm_output_cosine_similarity.csv\n",
            "LLM outputs saved to data/paraphases_with_llm_output.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Liquid AI's LFM2-1.2B"
      ],
      "metadata": {
        "id": "VN75O3XvMHeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rxTA-C69LjxM"
      }
    }
  ]
}