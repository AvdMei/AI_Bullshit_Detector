{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZz0QDqpeHWv3Mth8UGP/O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvdMei/AI_Bullshit_Detector/blob/ages_branch/AI_Bullshit_Detector_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Bullshit Detector\n",
        "\n",
        "Team:\n",
        "\n",
        "Attribution:\n",
        "\n",
        "LLM mode: LiquidAI LMF2-1.2B\n",
        "\n",
        "Sundai project 11-jan-26"
      ],
      "metadata": {
        "id": "93MCDWrzMSqa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUIbFdcKLfUv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from lion_pytorch import Lion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create template and tokenizer for LLM model to be able to chat with it"
      ],
      "metadata": {
        "id": "CbaGATN9ejrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic question-answer template\n",
        "template_without_answer = \"<|startoftext|><|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "template_with_answer = template_without_answer + \"{answer}<|im_end|>\\n\"\n",
        "\n",
        "# Let's try to put something into the template to see how it looks\n",
        "print(template_with_answer.format(question=\"What is your name?\", answer=\"My name is Lili!\"))"
      ],
      "metadata": {
        "id": "vfUiPGf4eXCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for Liquid AI LFM2-1.2B\n",
        "model_id = \"LiquidAI/LFM2-1.2B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# How big is the tokenizer?\n",
        "print(f\"Vocab size: {len(tokenizer.get_vocab())}\")"
      ],
      "metadata": {
        "id": "GulkJu1ieeKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets test out both steps:\n",
        "text = \"Here is some sample text!\"\n",
        "print(f\"Original text: {text}\")\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "print(f\"Encoded tokens: {tokens}\")\n",
        "\n",
        "# Decode the tokens\n",
        "decoded_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "print(f\"Decoded text: {decoded_text}\")"
      ],
      "metadata": {
        "id": "Bq0suq7Teip2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template_without_answer.format(question=\"What is the capital of France? Use one word.\")\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "rouLHxijeyB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model -- note that this may take a few minutes\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "Qulnj7PyeWpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Putting it together to prompt the model and generate a response ###\n",
        "\n",
        "# 1. Construct the prompt in chat template form\n",
        "question = \"What is the capital of France? Use one word.\"\n",
        "prompt = template_without_answer.format(question=question)\n",
        "# prompt = template_without_answer.format('''TODO''') # TODO\n",
        "\n",
        "# 2. Tokenize the prompt\n",
        "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# 3. Feed through the model to predict the next token probabilities\n",
        "with torch.no_grad():\n",
        "    output = model(tokens)\n",
        "    # output = '''TODO''' # TODO\n",
        "\n",
        "    probs = F.softmax(output.logits, dim=-1)\n",
        "\n",
        "# 4. Get the next token, according to the maximum probability\n",
        "next_token = torch.argmax(probs[0, -1, :]).item()\n",
        "\n",
        "# 5. Decode the next token\n",
        "next_token_text = tokenizer.decode(next_token)\n",
        "# next_token_text = '''TODO''' # TODO\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Predicted next token: {next_token_text}\")"
      ],
      "metadata": {
        "id": "JDbqR4eht0EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Liquid AI's LFM2-1.2B"
      ],
      "metadata": {
        "id": "VN75O3XvMHeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rxTA-C69LjxM"
      }
    }
  ]
}